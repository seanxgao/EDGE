{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Graph Memory System Tests\n",
        "\n",
        "This notebook provides deterministic smoke tests for the graph memory system.\n",
        "\n",
        "Tests include:\n",
        "- Basic operations (add and retrieve memories)\n",
        "- Store and load from file (persistence)\n",
        "- Store 100 memories and verify graph structure\n",
        "\n",
        "All tests use deterministic mock embeddings (no API calls) and fixed seeds for reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import tempfile\n",
        "import numpy as np\n",
        "from unittest.mock import patch\n",
        "\n",
        "# Add project root to path\n",
        "sys.path.insert(0, os.path.dirname(os.path.abspath('.')))\n",
        "\n",
        "from graph.graph_memory import GraphMemory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper: Mock Embedding\n",
        "\n",
        "Create deterministic mock embeddings for testing (no API calls needed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mock_embedding(text: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Create deterministic mock embedding based on text.\n",
        "    \n",
        "    Uses text hash as seed for reproducibility.\n",
        "    Returns normalized vector of shape (1536,).\n",
        "    \"\"\"\n",
        "    seed = abs(hash(text)) % 10000\n",
        "    rng = np.random.RandomState(seed)\n",
        "    vec = rng.randn(1536)\n",
        "    norm = np.linalg.norm(vec)\n",
        "    if norm == 0:\n",
        "        raise RuntimeError(\"Zero-norm embedding vector\")\n",
        "    return vec / norm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 1: Basic Operations\n",
        "\n",
        "Test adding a few memories and retrieving them\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Test 1: Basic Operations\n",
            "============================================================\n",
            "\n",
            "Adding 3 memories...\n",
            "  1. Added memory 1: I like coffee\n",
            "  2. Added memory 2: Python programming\n",
            "  3. Added memory 3: Machine learning\n",
            "\n",
            "Graph statistics:\n",
            "  Nodes: 3\n",
            "  Edges: 3\n",
            "  Density: 1.0\n",
            "\n",
            "Retrieving memories for query: 'programming'...\n",
            "  Found 2 results:\n",
            "    1. Python programming\n",
            "    2. I like coffee\n",
            "\n",
            "Test 1 PASSED\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Test 1: Basic Operations\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create temporary file\n",
        "fd, temp_path = tempfile.mkstemp(suffix='.json')\n",
        "os.close(fd)\n",
        "\n",
        "try:\n",
        "    with patch('graph.graph_memory.get_embedding', side_effect=mock_embedding):\n",
        "        memory = GraphMemory(temp_path)\n",
        "        \n",
        "        # Add memories\n",
        "        texts = [\"I like coffee\", \"Python programming\", \"Machine learning\"]\n",
        "        print(f\"\\nAdding {len(texts)} memories...\")\n",
        "        \n",
        "        for i, text in enumerate(texts, 1):\n",
        "            node_id = memory.add_memory(text)\n",
        "            print(f\"  {i}. Added memory {node_id}: {text}\")\n",
        "            assert node_id > 0, f\"Invalid node ID: {node_id}\"\n",
        "        \n",
        "        # Verify storage\n",
        "        stats = memory.show_stats()\n",
        "        print(f\"\\nGraph statistics:\")\n",
        "        print(f\"  Nodes: {stats['nodes']}\")\n",
        "        print(f\"  Edges: {stats['edges']}\")\n",
        "        print(f\"  Density: {stats['density']}\")\n",
        "        assert stats[\"nodes\"] == 3, f\"Expected 3 nodes, got {stats['nodes']}\"\n",
        "        assert stats[\"edges\"] >= 0, \"Invalid edge count\"\n",
        "        assert 0.0 <= stats[\"density\"] <= 1.0, f\"Invalid density: {stats['density']}\"\n",
        "        \n",
        "        # Retrieve memories\n",
        "        print(f\"\\nRetrieving memories for query: 'programming'...\")\n",
        "        results = memory.retrieve_memories(\"programming\", k=2)\n",
        "        print(f\"  Found {len(results)} results:\")\n",
        "        for i, result in enumerate(results, 1):\n",
        "            print(f\"    {i}. {result}\")\n",
        "        \n",
        "        assert len(results) == 2, f\"Expected 2 results, got {len(results)}\"\n",
        "        assert all(isinstance(r, str) for r in results), \"All results must be strings\"\n",
        "        assert all(len(r) > 0 for r in results), \"All results must be non-empty\"\n",
        "        \n",
        "        print(\"\\nTest 1 PASSED\")\n",
        "        \n",
        "finally:\n",
        "    # Clean up\n",
        "    if os.path.exists(temp_path):\n",
        "        os.remove(temp_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 2: Store and Load\n",
        "\n",
        "Test storing memories and loading them back from file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Test 2: Store and Load\n",
            "============================================================\n",
            "\n",
            "Session 1: Storing memories...\n",
            "  1. Stored memory 1: Coffee is good\n",
            "  2. Stored memory 2: Python programming\n",
            "  3. Stored memory 3: Machine learning\n",
            "\n",
            "  Stored 3 memories\n",
            "  Memory file created: C:\\Users\\xeangao\\AppData\\Local\\Temp\\tmpef464meb.json\n",
            "\n",
            "Session 2: Loading memories from file...\n",
            "  Loaded 3 nodes, 3 edges\n",
            "\n",
            "  Verifying stored texts...\n",
            "    Node 1: Coffee is good\n",
            "    Node 2: Python programming\n",
            "    Node 3: Machine learning\n",
            "\n",
            "  Verifying stored embeddings...\n",
            "\n",
            "  Testing retrieval...\n",
            "    Query: 'programming' -> Found: Coffee is good\n",
            "\n",
            "Test 2 PASSED\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Test 2: Store and Load\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create temporary file\n",
        "fd, temp_path = tempfile.mkstemp(suffix='.json')\n",
        "os.close(fd)\n",
        "\n",
        "try:\n",
        "    with patch('graph.graph_memory.get_embedding', side_effect=mock_embedding):\n",
        "        # First session: add memories\n",
        "        print(\"\\nSession 1: Storing memories...\")\n",
        "        memory1 = GraphMemory(temp_path)\n",
        "        texts = [\"Coffee is good\", \"Python programming\", \"Machine learning\"]\n",
        "        \n",
        "        node_ids = []\n",
        "        for i, text in enumerate(texts, 1):\n",
        "            node_id = memory1.add_memory(text)\n",
        "            node_ids.append(node_id)\n",
        "            print(f\"  {i}. Stored memory {node_id}: {text}\")\n",
        "        \n",
        "        print(f\"\\n  Stored {len(texts)} memories\")\n",
        "        \n",
        "        # Verify file was created\n",
        "        assert os.path.exists(temp_path), \"Memory file was not created\"\n",
        "        print(f\"  Memory file created: {temp_path}\")\n",
        "        \n",
        "        # Second session: load memories\n",
        "        print(\"\\nSession 2: Loading memories from file...\")\n",
        "        memory2 = GraphMemory(temp_path)\n",
        "        stats = memory2.show_stats()\n",
        "        \n",
        "        print(f\"  Loaded {stats['nodes']} nodes, {stats['edges']} edges\")\n",
        "        assert stats[\"nodes\"] == 3, f\"Expected 3 nodes, got {stats['nodes']}\"\n",
        "        assert stats[\"edges\"] >= 0, \"Invalid edge count\"\n",
        "        \n",
        "        # Verify texts are preserved\n",
        "        print(\"\\n  Verifying stored texts...\")\n",
        "        for node_id in node_ids:\n",
        "            text = memory2.store.get_node_text(node_id)\n",
        "            print(f\"    Node {node_id}: {text}\")\n",
        "            assert text in texts, f\"Text mismatch for node {node_id}\"\n",
        "        \n",
        "        # Verify embeddings are preserved\n",
        "        print(\"\\n  Verifying stored embeddings...\")\n",
        "        for node_id in node_ids:\n",
        "            embedding = memory2.store.get_node_embedding(node_id)\n",
        "            assert embedding.shape == (1536,), f\"Invalid embedding shape: {embedding.shape}\"\n",
        "            assert not np.any(np.isnan(embedding)), \"Embedding contains NaN\"\n",
        "            assert not np.any(np.isinf(embedding)), \"Embedding contains Inf\"\n",
        "        \n",
        "        # Test retrieval\n",
        "        print(\"\\n  Testing retrieval...\")\n",
        "        results = memory2.retrieve_memories(\"programming\", k=1)\n",
        "        print(f\"    Query: 'programming' -> Found: {results[0]}\")\n",
        "        assert len(results) >= 1, \"Retrieval returned no results\"\n",
        "        \n",
        "        print(\"\\nTest 2 PASSED\")\n",
        "        \n",
        "finally:\n",
        "    if os.path.exists(temp_path):\n",
        "        os.remove(temp_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 3: Store 99 Memories (Bacon Essays)\n",
        "\n",
        "Test storing 99 sentences from Bacon's first 3 essays and verify they are all saved correctly\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Test 3: Store 99 Memories (Bacon Essays)\n",
            "============================================================\n",
            "\n",
            "Adding 99 memories from 3essay.txt...\n",
            "  Progress: 20/99 memories added...\n",
            "  Progress: 40/99 memories added...\n",
            "  Progress: 60/99 memories added...\n",
            "  Progress: 80/99 memories added...\n",
            "\n",
            "  All 99 memories added!\n",
            "\n",
            "Graph statistics:\n",
            "  Nodes: 99\n",
            "  Edges: 291\n",
            "  Density: 0.06\n",
            "\n",
            "  All 99 nodes stored correctly\n",
            "  Node IDs correct: 1 to 99\n",
            "\n",
            "  Verifying embeddings...\n",
            "  Verified first 10 embeddings are valid\n",
            "\n",
            "  Verifying node retrieval...\n",
            "    Node 1: What is truth? said jesting Pilate, and would not ...\n",
            "    Node 2: Certainly there be, that delight in giddiness, and...\n",
            "    Node 3: And though the sects of philosophers of that kind ...\n",
            "    Node 4: But it is not only the difficulty and labor, which...\n",
            "    Node 5: One of the later school of the Grecians, examineth...\n",
            "\n",
            "  Testing retrieval with query: 'truth'...\n",
            "  Found 5 results:\n",
            "    1. Therefore it is most necessary, that the church, by doctrine...\n",
            "    2. Surely in counsels concerning religion, that counsel of the ...\n",
            "    3. There appear to be two extremes....\n",
            "    4. It is as natural to die, as to be born; and to a little infa...\n",
            "    5. And it was a notable observation of a wise father, and no le...\n",
            "\n",
            "Test 3 PASSED\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Test 3: Store 99 Memories (Bacon Essays)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create temporary file\n",
        "fd, temp_path = tempfile.mkstemp(suffix='.json')\n",
        "os.close(fd)\n",
        "\n",
        "try:\n",
        "    with patch('graph.graph_memory.get_embedding', side_effect=mock_embedding):\n",
        "        memory = GraphMemory(temp_path)\n",
        "        \n",
        "        # Load texts from 3essay.txt (99 sentences from first 3 essays)\n",
        "        essay_file = \"3essay.txt\"\n",
        "        with open(essay_file, 'r', encoding='utf-8') as f:\n",
        "            texts = [line.strip() for line in f if line.strip()]\n",
        "        \n",
        "        assert len(texts) == 99, f\"Expected 99 texts, got {len(texts)}\"\n",
        "        node_ids = []\n",
        "        \n",
        "        print(f\"\\nAdding {len(texts)} memories from {essay_file}...\")\n",
        "        for i, text in enumerate(texts, 1):\n",
        "            node_id = memory.add_memory(text)\n",
        "            node_ids.append(node_id)\n",
        "            \n",
        "            # Show progress every 20 memories\n",
        "            if i % 20 == 0:\n",
        "                print(f\"  Progress: {i}/{len(texts)} memories added...\")\n",
        "        \n",
        "        print(f\"\\n  All {len(texts)} memories added!\")\n",
        "        \n",
        "        # Verify all nodes were added\n",
        "        stats = memory.show_stats()\n",
        "        print(f\"\\nGraph statistics:\")\n",
        "        print(f\"  Nodes: {stats['nodes']}\")\n",
        "        print(f\"  Edges: {stats['edges']}\")\n",
        "        print(f\"  Density: {stats['density']}\")\n",
        "        \n",
        "        assert stats[\"nodes\"] == 99, f\"Expected 99 nodes, got {stats['nodes']}\"\n",
        "        assert stats[\"edges\"] >= 0, \"Invalid edge count\"\n",
        "        assert 0.0 <= stats[\"density\"] <= 1.0, f\"Invalid density: {stats['density']}\"\n",
        "        print(f\"\\n  All {len(texts)} nodes stored correctly\")\n",
        "        \n",
        "        # Verify node IDs\n",
        "        assert min(node_ids) == 1, f\"Invalid min node ID: {min(node_ids)}\"\n",
        "        assert max(node_ids) == 99, f\"Invalid max node ID: {max(node_ids)}\"\n",
        "        assert len(set(node_ids)) == 99, \"Duplicate node IDs found\"\n",
        "        print(f\"  Node IDs correct: {min(node_ids)} to {max(node_ids)}\")\n",
        "        \n",
        "        # Verify all embeddings are valid\n",
        "        print(f\"\\n  Verifying embeddings...\")\n",
        "        for node_id in node_ids[:10]:  # Check first 10\n",
        "            embedding = memory.store.get_node_embedding(node_id)\n",
        "            assert embedding.shape == (1536,), f\"Invalid embedding shape for node {node_id}\"\n",
        "            assert not np.any(np.isnan(embedding)), f\"NaN in embedding for node {node_id}\"\n",
        "            assert not np.any(np.isinf(embedding)), f\"Inf in embedding for node {node_id}\"\n",
        "        print(f\"  Verified first 10 embeddings are valid\")\n",
        "        \n",
        "        # Verify we can retrieve nodes\n",
        "        print(f\"\\n  Verifying node retrieval...\")\n",
        "        for node_id in node_ids[:5]:  # Check first 5\n",
        "            text = memory.store.get_node_text(node_id)\n",
        "            assert text in texts, f\"Text not found for node {node_id}\"\n",
        "            print(f\"    Node {node_id}: {text[:50]}...\")\n",
        "        \n",
        "        # Test retrieval\n",
        "        print(f\"\\n  Testing retrieval with query: 'truth'...\")\n",
        "        results = memory.retrieve_memories(\"truth\", k=5)\n",
        "        print(f\"  Found {len(results)} results:\")\n",
        "        for i, result in enumerate(results, 1):\n",
        "            print(f\"    {i}. {result[:60]}...\")\n",
        "        \n",
        "        assert len(results) == 5, f\"Expected 5 results, got {len(results)}\"\n",
        "        assert all(isinstance(r, str) for r in results), \"All results must be strings\"\n",
        "        assert all(len(r) > 0 for r in results), \"All results must be non-empty\"\n",
        "        \n",
        "        print(\"\\nTest 3 PASSED\")\n",
        "        \n",
        "finally:\n",
        "    if os.path.exists(temp_path):\n",
        "        os.remove(temp_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 3: Graph Structure Analysis\n",
        "\n",
        "Display graph edges and compute Laplacian matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Graph Edge Information\n",
            "============================================================\n",
            "\n",
            "Total edges: 291\n",
            "\n",
            "First 10 edges (node_id1, node_id2, weight):\n",
            "  1. (1, 2, 0.014030)\n",
            "  2. (1, 3, 0.009871)\n",
            "  3. (1, 4, 0.036908)\n",
            "  4. (1, 5, 0.010852)\n",
            "  5. (1, 6, -0.012408)\n",
            "  6. (1, 7, -0.013408)\n",
            "  7. (1, 9, 0.021642)\n",
            "  8. (1, 10, 0.017066)\n",
            "  9. (1, 12, 0.043153)\n",
            "  10. (1, 13, 0.030813)\n",
            "\n",
            "============================================================\n",
            "Computing Laplacian Matrix\n",
            "============================================================\n",
            "\n",
            "Laplacian matrix shape: (99, 99)\n",
            "Min value: -0.089426\n",
            "Max value: 0.614703\n",
            "Trace (sum of diagonal): 25.712860\n",
            "Matrix is symmetric: True\n",
            "\n",
            "Laplacian matrix saved to: laplacian_matrix.npy\n",
            "Load in SCOPE project with: laplacian = np.load('laplacian_matrix.npy')\n"
          ]
        }
      ],
      "source": [
        "# Recreate the same graph structure for analysis\n",
        "fd, temp_path = tempfile.mkstemp(suffix='.json')\n",
        "os.close(fd)\n",
        "\n",
        "try:\n",
        "    with patch('graph.graph_memory.get_embedding', side_effect=mock_embedding):\n",
        "        memory = GraphMemory(temp_path)\n",
        "        \n",
        "        # Load texts from 3essay.txt (same as Test 3)\n",
        "        essay_file = \"3essay.txt\"\n",
        "        with open(essay_file, 'r', encoding='utf-8') as f:\n",
        "            texts = [line.strip() for line in f if line.strip()]\n",
        "        \n",
        "        assert len(texts) == 99, f\"Expected 99 texts, got {len(texts)}\"\n",
        "        \n",
        "        for text in texts:\n",
        "            memory.add_memory(text)\n",
        "        \n",
        "        # Display graph edges\n",
        "        print(\"=\" * 60)\n",
        "        print(\"Graph Edge Information\")\n",
        "        print(\"=\" * 60)\n",
        "        edges = list(memory.store.graph.edges(data=True))\n",
        "        print(f\"\\nTotal edges: {len(edges)}\")\n",
        "        print(f\"\\nFirst 10 edges (node_id1, node_id2, weight):\")\n",
        "        for i, (u, v, data) in enumerate(edges[:10], 1):\n",
        "            weight = data.get(\"weight\", 1.0)\n",
        "            print(f\"  {i}. ({u}, {v}, {weight:.6f})\")\n",
        "        \n",
        "        # Compute Laplacian matrix\n",
        "        print(f\"\\n\" + \"=\" * 60)\n",
        "        print(\"Computing Laplacian Matrix\")\n",
        "        print(\"=\" * 60)\n",
        "        import networkx as nx\n",
        "        graph = memory.store.graph\n",
        "        \n",
        "        # Get sorted node IDs for matrix ordering\n",
        "        node_ids = sorted(memory.store.get_all_node_ids())\n",
        "        n_nodes = len(node_ids)\n",
        "        \n",
        "        # Use NetworkX to compute weighted Laplacian matrix\n",
        "        laplacian_nx = nx.laplacian_matrix(graph, nodelist=node_ids, weight='weight')\n",
        "        laplacian = laplacian_nx.toarray().astype(np.float64)\n",
        "        \n",
        "        print(f\"\\nLaplacian matrix shape: {laplacian.shape}\")\n",
        "        print(f\"Min value: {np.min(laplacian):.6f}\")\n",
        "        print(f\"Max value: {np.max(laplacian):.6f}\")\n",
        "        print(f\"Trace (sum of diagonal): {np.trace(laplacian):.6f}\")\n",
        "        print(f\"Matrix is symmetric: {np.allclose(laplacian, laplacian.T)}\")\n",
        "        \n",
        "        # Save Laplacian matrix to file for SCOPE project\n",
        "        laplacian_file = \"laplacian_matrix.npy\"\n",
        "        np.save(laplacian_file, laplacian)\n",
        "        print(f\"\\nLaplacian matrix saved to: {laplacian_file}\")\n",
        "        print(f\"Load in SCOPE project with: laplacian = np.load('{laplacian_file}')\")\n",
        "        \n",
        "finally:\n",
        "    if os.path.exists(temp_path):\n",
        "        os.remove(temp_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SCOPE Output Visualization\n",
        "\n",
        "Visualization of the graph structure processed by SCOPE (initial result):\n",
        "\n",
        "![SCOPE Output](output.png)\n",
        "\n",
        "Note: This is an initial visualization result from SCOPE processing of the 99-node graph structure. The visualization shows the graph topology and structure derived from the Laplacian matrix.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "All tests completed. The graph memory system:\n",
        "- Can store and retrieve memories\n",
        "- Can persist memories to file and load them back\n",
        "- Can handle 100 memories correctly\n",
        "- All embeddings are valid (no NaN or Inf)\n",
        "- Graph structure is correct (node IDs, edges, density)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
