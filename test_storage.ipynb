{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Storage System Comprehensive Tests\n",
        "\n",
        "This notebook tests the new hybrid storage architecture (SQLite + JSONL + NumPy).\n",
        "\n",
        "Test coverage:\n",
        "1. Database initialization\n",
        "2. Node creation from existing data\n",
        "3. CRUD operations\n",
        "4. JSONL and NumPy file operations\n",
        "5. Graph edge operations\n",
        "6. Usage statistics updates\n",
        "7. Data consistency verification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All imports successful\n"
          ]
        }
      ],
      "source": [
        "# Setup and imports\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# Import memory modules\n",
        "from memory import (\n",
        "    init_db,\n",
        "    create_node,\n",
        "    get_node,\n",
        "    update_usage,\n",
        "    add_graph_edge,\n",
        "    load_sentence,\n",
        "    load_embedding,\n",
        "    load_edge_arrays,\n",
        "    count_sentences,\n",
        "    get_embedding_shape,\n",
        "    NodesContext,\n",
        "    Neighbors,\n",
        "    NodesUsage,\n",
        ")\n",
        "\n",
        "print(\"All imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 1: Database Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Database initialized: test_memory.db\n",
            "Tables created: ['neighbors', 'nodes_context', 'nodes_usage']\n",
            "✓ All tables created successfully\n"
          ]
        }
      ],
      "source": [
        "# Initialize database\n",
        "test_db_path = \"test_memory.db\"\n",
        "if os.path.exists(test_db_path):\n",
        "    os.remove(test_db_path)\n",
        "\n",
        "db = init_db(test_db_path)\n",
        "print(f\"Database initialized: {test_db_path}\")\n",
        "print(f\"Tables created: {db.get_tables()}\")\n",
        "\n",
        "# Verify tables exist\n",
        "assert \"nodes_context\" in db.get_tables()\n",
        "assert \"neighbors\" in db.get_tables()\n",
        "assert \"nodes_usage\" in db.get_tables()\n",
        "print(\"✓ All tables created successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 2: Load Existing Data and Create Nodes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 99 sentences in memory_bacon/core/sentences.jsonl\n",
            "  [0] bacon_001: What is truth? said jesting Pilate, and would not ...\n",
            "  [1] bacon_002: Certainly there be, that delight in giddiness, and...\n",
            "  [2] bacon_003: And though the sects of philosophers of that kind ...\n"
          ]
        }
      ],
      "source": [
        "# Check if existing data exists\n",
        "sentences_path = \"memory_bacon/core/sentences.jsonl\"\n",
        "embeddings_path = \"memory_bacon/core/embeddings.npy\"\n",
        "\n",
        "if not os.path.exists(sentences_path):\n",
        "    print(\"⚠ sentences.jsonl not found. Run parse_essay.py first.\")\n",
        "    print(\"Running parse_essay.py...\")\n",
        "    import subprocess\n",
        "    result = subprocess.run([\"python\", \"parse_essay.py\"], capture_output=True, text=True)\n",
        "    print(result.stdout)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"Error: {result.stderr}\")\n",
        "\n",
        "if not os.path.exists(embeddings_path):\n",
        "    print(\"⚠ embeddings.npy not found. Run generate_embeddings.py first.\")\n",
        "    print(\"Note: This requires OpenAI API key. Skipping embedding generation in test.\")\n",
        "\n",
        "# Load existing sentences\n",
        "if os.path.exists(sentences_path):\n",
        "    num_sentences = count_sentences(sentences_path)\n",
        "    print(f\"Found {num_sentences} sentences in {sentences_path}\")\n",
        "    \n",
        "    # Load first 3 sentences as sample\n",
        "    sample_sentences = []\n",
        "    for i in range(min(3, num_sentences)):\n",
        "        sent = load_sentence(sentences_path, i)\n",
        "        if sent:\n",
        "            sample_sentences.append(sent)\n",
        "            print(f\"  [{i}] {sent.get('id', 'N/A')}: {sent.get('text', '')[:50]}...\")\n",
        "else:\n",
        "    print(\"⚠ No existing sentences found\")\n",
        "    sample_sentences = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings shape: 99 sentences × 1536 dimensions\n",
            "Sample embedding norm: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Check embeddings\n",
        "if os.path.exists(embeddings_path):\n",
        "    shape = get_embedding_shape(embeddings_path)\n",
        "    if shape:\n",
        "        print(f\"Embeddings shape: {shape[0]} sentences × {shape[1]} dimensions\")\n",
        "        \n",
        "        # Load first embedding as sample\n",
        "        sample_embedding = load_embedding(embeddings_path, 0)\n",
        "        if sample_embedding is not None:\n",
        "            print(f\"Sample embedding norm: {np.linalg.norm(sample_embedding):.4f}\")\n",
        "    else:\n",
        "        print(\"⚠ Could not read embeddings shape\")\n",
        "else:\n",
        "    print(\"⚠ embeddings.npy not found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 3: Create Nodes from Existing Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating 5 node records from existing data...\n",
            "(Using direct DB operations to avoid modifying original files)\n",
            "  Created node 1: bacon_001\n",
            "  Created node 2: bacon_002\n",
            "  Created node 3: bacon_003\n",
            "  Created node 4: bacon_004\n",
            "  Created node 5: bacon_005\n",
            "✓ Created 5 node records\n"
          ]
        }
      ],
      "source": [
        "# Create nodes from existing data (if available)\n",
        "# Use direct database operations to avoid modifying original files\n",
        "created_node_ids = []\n",
        "\n",
        "if os.path.exists(sentences_path) and os.path.exists(embeddings_path):\n",
        "    # Load all sentences and embeddings\n",
        "    sentences = []\n",
        "    with open(sentences_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            sentences.append(json.loads(line))\n",
        "    \n",
        "    embeddings = np.load(embeddings_path, mmap_mode='r')\n",
        "    \n",
        "    # Create nodes for first 5 sentences (or all if less than 5)\n",
        "    # Use direct DB operations to point to existing file indices (don't modify files)\n",
        "    num_to_create = min(5, len(sentences), len(embeddings))\n",
        "    print(f\"Creating {num_to_create} node records from existing data...\")\n",
        "    print(\"(Using direct DB operations to avoid modifying original files)\")\n",
        "    \n",
        "    from memory.database import add_node_context, add_node_usage\n",
        "    from memory.npy_utils import compute_embedding_norm\n",
        "    \n",
        "    for i in range(num_to_create):\n",
        "        sent = sentences[i]\n",
        "        embedding = embeddings[i]\n",
        "        node_id = i + 1  # Use 1-based node IDs\n",
        "        \n",
        "        # Compute embedding norm\n",
        "        embedding_norm = compute_embedding_norm(embedding)\n",
        "        \n",
        "        # Add node context (pointing to existing file indices)\n",
        "        add_node_context(\n",
        "            node_id=node_id,\n",
        "            sentence_offset=i,  # Index into existing sentences.jsonl\n",
        "            embedding_index=i,  # Index into existing embeddings.npy\n",
        "            source='3essay.txt',\n",
        "            tag=f\"chapter_{sent.get('chapter', 1)}\",\n",
        "            language='en',\n",
        "            initial_context=sent.get('text', '')[:100],\n",
        "            embedding_norm=embedding_norm,\n",
        "        )\n",
        "        \n",
        "        # Add node usage (default values)\n",
        "        add_node_usage(node_id=node_id)\n",
        "        \n",
        "        created_node_ids.append(node_id)\n",
        "        print(f\"  Created node {node_id}: {sent.get('id', 'N/A')}\")\n",
        "    \n",
        "    print(f\"✓ Created {len(created_node_ids)} node records\")\n",
        "else:\n",
        "    print(\"⚠ Skipping node creation (missing data files)\")\n",
        "    # Create a test node with dummy data using create_node (this will create new files)\n",
        "    print(\"Creating test node with dummy data (using create_node)...\")\n",
        "    test_sentences_path = \"test_sentences.jsonl\"\n",
        "    test_embeddings_path = \"test_embeddings.npy\"\n",
        "    \n",
        "    # Clean up test files if they exist\n",
        "    if os.path.exists(test_sentences_path):\n",
        "        os.remove(test_sentences_path)\n",
        "    if os.path.exists(test_embeddings_path):\n",
        "        os.remove(test_embeddings_path)\n",
        "    \n",
        "    dummy_embedding = np.random.rand(1536).astype(np.float32)\n",
        "    node_id = create_node(\n",
        "        sentence_text=\"Test sentence for storage system.\",\n",
        "        embedding_vector=dummy_embedding,\n",
        "        metadata_dict={\n",
        "            'id': 1,\n",
        "            'source': 'test',\n",
        "            'tag': 'test',\n",
        "            'language': 'en',\n",
        "        },\n",
        "        sentences_path=test_sentences_path,\n",
        "        embeddings_path=test_embeddings_path,\n",
        "        db_path=test_db_path,\n",
        "    )\n",
        "    created_node_ids.append(node_id)\n",
        "    print(f\"✓ Created test node {node_id}\")\n",
        "    \n",
        "    # Update paths for subsequent tests\n",
        "    sentences_path = test_sentences_path\n",
        "    embeddings_path = test_embeddings_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 4: Retrieve Nodes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing get_node()...\n",
            "\n",
            "Node 1:\n",
            "  Context: id=1, sentence_offset=0\n",
            "  Usage: access_count=0, popularity=0.0\n",
            "  Neighbors: 0 edges\n",
            "  Sentence: What is truth? said jesting Pilate, and would not stay for a...\n",
            "  Embedding: 1536 dimensions\n",
            "  ✓ Data consistency verified\n",
            "\n",
            "Node 2:\n",
            "  Context: id=2, sentence_offset=1\n",
            "  Usage: access_count=0, popularity=0.0\n",
            "  Neighbors: 0 edges\n",
            "  Sentence: Certainly there be, that delight in giddiness, and count it ...\n",
            "  Embedding: 1536 dimensions\n",
            "  ✓ Data consistency verified\n",
            "\n",
            "Node 3:\n",
            "  Context: id=3, sentence_offset=2\n",
            "  Usage: access_count=0, popularity=0.0\n",
            "  Neighbors: 0 edges\n",
            "  Sentence: And though the sects of philosophers of that kind be gone, y...\n",
            "  Embedding: 1536 dimensions\n",
            "  ✓ Data consistency verified\n",
            "\n",
            "Node 4:\n",
            "  Context: id=4, sentence_offset=3\n",
            "  Usage: access_count=0, popularity=0.0\n",
            "  Neighbors: 0 edges\n",
            "  Sentence: But it is not only the difficulty and labor, which men take ...\n",
            "  Embedding: 1536 dimensions\n",
            "  ✓ Data consistency verified\n",
            "\n",
            "Node 5:\n",
            "  Context: id=5, sentence_offset=4\n",
            "  Usage: access_count=0, popularity=0.0\n",
            "  Neighbors: 0 edges\n",
            "  Sentence: One of the later school of the Grecians, examineth the matte...\n",
            "  Embedding: 1536 dimensions\n",
            "  ✓ Data consistency verified\n",
            "\n",
            "✓ All nodes retrieved successfully\n"
          ]
        }
      ],
      "source": [
        "# Test get_node for each created node\n",
        "print(\"Testing get_node()...\")\n",
        "for node_id in created_node_ids:\n",
        "    node = get_node(\n",
        "        node_id,\n",
        "        sentences_path=sentences_path,\n",
        "        embeddings_path=embeddings_path,\n",
        "        db_path=test_db_path,\n",
        "    )\n",
        "    \n",
        "    if node:\n",
        "        print(f\"\\nNode {node_id}:\")\n",
        "        print(f\"  Context: id={node['context']['id']}, sentence_offset={node['context']['sentence_offset']}\")\n",
        "        print(f\"  Usage: access_count={node['usage']['access_count']}, popularity={node['usage']['popularity']}\")\n",
        "        print(f\"  Neighbors: {len(node['neighbors'])} edges\")\n",
        "        if node['sentence']:\n",
        "            print(f\"  Sentence: {node['sentence'].get('text', '')[:60]}...\")\n",
        "        if node['embedding']:\n",
        "            print(f\"  Embedding: {len(node['embedding'])} dimensions\")\n",
        "        \n",
        "        # Verify data consistency\n",
        "        assert node['context'] is not None, \"Context should not be None\"\n",
        "        assert node['usage'] is not None, \"Usage should not be None\"\n",
        "        assert node['sentence'] is not None, \"Sentence should not be None\"\n",
        "        assert node['embedding'] is not None, \"Embedding should not be None\"\n",
        "        print(f\"  ✓ Data consistency verified\")\n",
        "    else:\n",
        "        print(f\"  ✗ Node {node_id} not found\")\n",
        "\n",
        "print(\"\\n✓ All nodes retrieved successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 5: Update Usage Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing update_usage()...\n",
            "Initial access_count: 0\n",
            "✓ update_usage returned: True\n",
            "Updated access_count: 1\n",
            "✓ Usage statistics updated correctly\n"
          ]
        }
      ],
      "source": [
        "# Test update_usage\n",
        "print(\"Testing update_usage()...\")\n",
        "test_node_id = created_node_ids[0]\n",
        "\n",
        "# Get initial usage\n",
        "initial_node = get_node(test_node_id, db_path=test_db_path)\n",
        "initial_access = initial_node['usage']['access_count']\n",
        "print(f\"Initial access_count: {initial_access}\")\n",
        "\n",
        "# Update usage\n",
        "success = update_usage(\n",
        "    node_id=test_node_id,\n",
        "    access_count=initial_access + 1,\n",
        "    last_access_time=datetime.now(),\n",
        "    recent_hit_count=3,\n",
        "    decay_score=0.85,\n",
        "    popularity=0.92,\n",
        "    db_path=test_db_path,\n",
        ")\n",
        "\n",
        "assert success, \"update_usage should return True\"\n",
        "print(f\"✓ update_usage returned: {success}\")\n",
        "\n",
        "# Verify update\n",
        "updated_node = get_node(test_node_id, db_path=test_db_path)\n",
        "updated_access = updated_node['usage']['access_count']\n",
        "print(f\"Updated access_count: {updated_access}\")\n",
        "assert updated_access == initial_access + 1, \"access_count should be incremented\"\n",
        "print(\"✓ Usage statistics updated correctly\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 6: Graph Edge Operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing add_graph_edge()...\n",
            "  Added edge: 1 -> 2, type=adjacent, weight=1.00\n",
            "  Added edge: 2 -> 3, type=same_chapter, weight=0.90\n",
            "  Added edge: 3 -> 4, type=question_context, weight=0.80\n",
            "✓ Added 3 edges\n",
            "\n",
            "Verifying edges...\n",
            "  ✓ Edge 1 -> 2 verified\n",
            "  ✓ Edge 2 -> 3 verified\n",
            "  ✓ Edge 3 -> 4 verified\n",
            "✓ All edges verified\n"
          ]
        }
      ],
      "source": [
        "# Test add_graph_edge\n",
        "print(\"Testing add_graph_edge()...\")\n",
        "\n",
        "if len(created_node_ids) >= 2:\n",
        "    # Add edges between nodes\n",
        "    edge_types = {\n",
        "        0: \"adjacent\",\n",
        "        1: \"same_chapter\",\n",
        "        2: \"question_context\",\n",
        "        3: \"definition_context\",\n",
        "    }\n",
        "    \n",
        "    # Add a few edges\n",
        "    edges_added = []\n",
        "    for i in range(min(3, len(created_node_ids) - 1)):\n",
        "        u = created_node_ids[i]\n",
        "        v = created_node_ids[i + 1]\n",
        "        edge_type = i % 4\n",
        "        weight = 1.0 - (i * 0.1)\n",
        "        \n",
        "        success = add_graph_edge(u, v, weight, edge_type, db_path=test_db_path)\n",
        "        assert success, f\"add_graph_edge should return True for ({u}, {v})\"\n",
        "        edges_added.append((u, v, weight, edge_type))\n",
        "        print(f\"  Added edge: {u} -> {v}, type={edge_types[edge_type]}, weight={weight:.2f}\")\n",
        "    \n",
        "    print(f\"✓ Added {len(edges_added)} edges\")\n",
        "    \n",
        "    # Verify edges by querying neighbors\n",
        "    print(\"\\nVerifying edges...\")\n",
        "    for u, v, weight, edge_type in edges_added:\n",
        "        node = get_node(u, db_path=test_db_path)\n",
        "        neighbors = node['neighbors']\n",
        "        \n",
        "        # Check if edge exists\n",
        "        found = any(n['v'] == v and n['edge_type'] == edge_type for n in neighbors)\n",
        "        assert found, f\"Edge ({u}, {v}) should exist in neighbors\"\n",
        "        print(f\"  ✓ Edge {u} -> {v} verified\")\n",
        "    \n",
        "    print(\"✓ All edges verified\")\n",
        "else:\n",
        "    print(\"⚠ Need at least 2 nodes to test edges. Skipping.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 7: Load Edge Arrays from NumPy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing load_edge_arrays() from memory_bacon/graph...\n",
            "  edge_index shape: (2, 201)\n",
            "  edge_weight shape: (201,)\n",
            "  edge_type shape: (201,)\n",
            "  Total edges: 201\n",
            "  Edge types: [0 2 3]\n",
            "✓ Edge arrays loaded successfully\n"
          ]
        }
      ],
      "source": [
        "# Test load_edge_arrays\n",
        "graph_dir = \"memory_bacon/graph\"\n",
        "print(f\"Testing load_edge_arrays() from {graph_dir}...\")\n",
        "\n",
        "edge_arrays = load_edge_arrays(graph_dir)\n",
        "if edge_arrays:\n",
        "    edge_index, edge_weight, edge_type = edge_arrays\n",
        "    print(f\"  edge_index shape: {edge_index.shape}\")\n",
        "    print(f\"  edge_weight shape: {edge_weight.shape}\")\n",
        "    print(f\"  edge_type shape: {edge_type.shape}\")\n",
        "    \n",
        "    # Verify consistency\n",
        "    assert len(edge_weight) == len(edge_type), \"edge_weight and edge_type should have same length\"\n",
        "    assert edge_index.shape[0] == 2, \"edge_index should have 2 rows (source, target)\"\n",
        "    assert edge_index.shape[1] == len(edge_weight), \"edge_index columns should match edge count\"\n",
        "    \n",
        "    print(f\"  Total edges: {len(edge_weight)}\")\n",
        "    print(f\"  Edge types: {np.unique(edge_type)}\")\n",
        "    print(\"✓ Edge arrays loaded successfully\")\n",
        "else:\n",
        "    print(\"⚠ Edge arrays not found. Run convert_edges_to_numpy.py to generate them.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 8: Database Query Operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing direct database queries...\n",
            "Total nodes in database: 5\n",
            "Total edges in database: 3\n",
            "Total usage records: 5\n",
            "Nodes with tags: 5\n",
            "Nodes with tag 'chapter_1': 5\n",
            "Nodes with source: 5\n",
            "Neighbors of node 1: 1\n",
            "  Outgoing edges from node 1:\n",
            "    -> 2 (weight=1.00, type=0)\n",
            "Oldest node: 1 (created: 2025-11-13 15:18:53)\n",
            "Newest node: 1 (created: 2025-11-13 15:18:53)\n",
            "✓ Database queries successful\n"
          ]
        }
      ],
      "source": [
        "# Test direct database queries\n",
        "print(\"Testing direct database queries...\")\n",
        "\n",
        "# Count nodes\n",
        "node_count = NodesContext.select().count()\n",
        "print(f\"Total nodes in database: {node_count}\")\n",
        "assert node_count == len(created_node_ids), \"Node count should match created nodes\"\n",
        "\n",
        "# Count edges\n",
        "edge_count = Neighbors.select().count()\n",
        "print(f\"Total edges in database: {edge_count}\")\n",
        "\n",
        "# Count usage records\n",
        "usage_count = NodesUsage.select().count()\n",
        "print(f\"Total usage records: {usage_count}\")\n",
        "assert usage_count == len(created_node_ids), \"Usage count should match node count\"\n",
        "\n",
        "# Query nodes with specific criteria\n",
        "# Use is_null(False) instead of is_not(None) in Peewee\n",
        "nodes_with_tag = NodesContext.select().where(NodesContext.tag.is_null(False)).count()\n",
        "print(f\"Nodes with tags: {nodes_with_tag}\")\n",
        "\n",
        "# Query nodes with specific tag value\n",
        "if nodes_with_tag > 0:\n",
        "    test_tag = NodesContext.select().where(NodesContext.tag.is_null(False)).first()\n",
        "    if test_tag:\n",
        "        nodes_with_specific_tag = NodesContext.select().where(NodesContext.tag == test_tag.tag).count()\n",
        "        print(f\"Nodes with tag '{test_tag.tag}': {nodes_with_specific_tag}\")\n",
        "\n",
        "# Query nodes with source\n",
        "nodes_with_source = NodesContext.select().where(NodesContext.source.is_null(False)).count()\n",
        "print(f\"Nodes with source: {nodes_with_source}\")\n",
        "\n",
        "# Query neighbors for a specific node\n",
        "if len(created_node_ids) > 0:\n",
        "    test_node_id = created_node_ids[0]\n",
        "    neighbor_count = Neighbors.select().where(Neighbors.u == test_node_id).count()\n",
        "    print(f\"Neighbors of node {test_node_id}: {neighbor_count}\")\n",
        "    \n",
        "    # Query outgoing edges\n",
        "    outgoing_edges = Neighbors.select().where(Neighbors.u == test_node_id)\n",
        "    print(f\"  Outgoing edges from node {test_node_id}:\")\n",
        "    for edge in outgoing_edges:\n",
        "        print(f\"    -> {edge.v} (weight={edge.weight:.2f}, type={edge.edge_type})\")\n",
        "\n",
        "# Query nodes ordered by creation time\n",
        "oldest_node = NodesContext.select().order_by(NodesContext.created_at.asc()).first()\n",
        "newest_node = NodesContext.select().order_by(NodesContext.created_at.desc()).first()\n",
        "if oldest_node and newest_node:\n",
        "    print(f\"Oldest node: {oldest_node.id} (created: {oldest_node.created_at})\")\n",
        "    print(f\"Newest node: {newest_node.id} (created: {newest_node.created_at})\")\n",
        "\n",
        "print(\"✓ Database queries successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 9: Data Consistency Verification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running data consistency checks...\n",
            "  Checking node-usage consistency...\n",
            "  Checking sentence offset validity...\n",
            "  Checking embedding index validity...\n",
            "  Checking neighbor reference validity...\n",
            "    Checked 3 edges\n",
            "  Checking get_node() integrity...\n",
            "\n",
            "✓ All consistency checks passed\n"
          ]
        }
      ],
      "source": [
        "# Comprehensive consistency check\n",
        "print(\"Running data consistency checks...\")\n",
        "\n",
        "# Ensure database is initialized\n",
        "init_db(test_db_path)\n",
        "\n",
        "errors = []\n",
        "\n",
        "# Check 1: All nodes have usage records\n",
        "print(\"  Checking node-usage consistency...\")\n",
        "for node_id in created_node_ids:\n",
        "    try:\n",
        "        context = NodesContext.get_by_id(node_id)\n",
        "        try:\n",
        "            usage = NodesUsage.get_by_id(node_id)\n",
        "        except NodesUsage.DoesNotExist:\n",
        "            errors.append(f\"Node {node_id} missing usage record\")\n",
        "    except NodesContext.DoesNotExist:\n",
        "        errors.append(f\"Node {node_id} does not exist in context table\")\n",
        "\n",
        "# Check 2: Sentence offsets are valid\n",
        "print(\"  Checking sentence offset validity...\")\n",
        "if os.path.exists(sentences_path):\n",
        "    max_sentence_count = count_sentences(sentences_path)\n",
        "    for node_id in created_node_ids:\n",
        "        try:\n",
        "            context = NodesContext.get_by_id(node_id)\n",
        "            if context.sentence_offset < 0:\n",
        "                errors.append(f\"Node {node_id} has negative sentence_offset {context.sentence_offset}\")\n",
        "            elif context.sentence_offset >= max_sentence_count:\n",
        "                errors.append(f\"Node {node_id} has invalid sentence_offset {context.sentence_offset} (max: {max_sentence_count - 1})\")\n",
        "        except NodesContext.DoesNotExist:\n",
        "            errors.append(f\"Node {node_id} does not exist (cannot check sentence_offset)\")\n",
        "\n",
        "# Check 3: Embedding indices are valid\n",
        "print(\"  Checking embedding index validity...\")\n",
        "if os.path.exists(embeddings_path):\n",
        "    shape = get_embedding_shape(embeddings_path)\n",
        "    if shape:\n",
        "        max_embedding_count = shape[0]\n",
        "        for node_id in created_node_ids:\n",
        "            try:\n",
        "                context = NodesContext.get_by_id(node_id)\n",
        "                if context.embedding_index < 0:\n",
        "                    errors.append(f\"Node {node_id} has negative embedding_index {context.embedding_index}\")\n",
        "                elif context.embedding_index >= max_embedding_count:\n",
        "                    errors.append(f\"Node {node_id} has invalid embedding_index {context.embedding_index} (max: {max_embedding_count - 1})\")\n",
        "            except NodesContext.DoesNotExist:\n",
        "                errors.append(f\"Node {node_id} does not exist (cannot check embedding_index)\")\n",
        "    else:\n",
        "        print(\"    ⚠ Could not read embedding shape\")\n",
        "else:\n",
        "    print(\"    ⚠ Embeddings file not found, skipping embedding index check\")\n",
        "\n",
        "# Check 4: Neighbor references are valid\n",
        "print(\"  Checking neighbor reference validity...\")\n",
        "edge_count = 0\n",
        "for edge in Neighbors.select():\n",
        "    edge_count += 1\n",
        "    try:\n",
        "        NodesContext.get_by_id(edge.u)\n",
        "    except NodesContext.DoesNotExist:\n",
        "        errors.append(f\"Edge ({edge.u}, {edge.v}) references non-existent source node {edge.u}\")\n",
        "    try:\n",
        "        NodesContext.get_by_id(edge.v)\n",
        "    except NodesContext.DoesNotExist:\n",
        "        errors.append(f\"Edge ({edge.u}, {edge.v}) references non-existent target node {edge.v}\")\n",
        "\n",
        "print(f\"    Checked {edge_count} edges\")\n",
        "\n",
        "# Check 5: Data integrity - verify get_node works for all nodes\n",
        "print(\"  Checking get_node() integrity...\")\n",
        "for node_id in created_node_ids:\n",
        "    try:\n",
        "        node = get_node(node_id, sentences_path=sentences_path, embeddings_path=embeddings_path, db_path=test_db_path)\n",
        "        if node is None:\n",
        "            errors.append(f\"get_node({node_id}) returned None\")\n",
        "        elif node['context'] is None:\n",
        "            errors.append(f\"get_node({node_id}) has None context\")\n",
        "        elif node['usage'] is None:\n",
        "            errors.append(f\"get_node({node_id}) has None usage\")\n",
        "    except Exception as e:\n",
        "        errors.append(f\"get_node({node_id}) raised exception: {e}\")\n",
        "\n",
        "if errors:\n",
        "    print(f\"\\n✗ Found {len(errors)} consistency errors:\")\n",
        "    for error in errors:\n",
        "        print(f\"  - {error}\")\n",
        "    raise AssertionError(f\"Data consistency check failed with {len(errors)} errors\")\n",
        "else:\n",
        "    print(\"\\n✓ All consistency checks passed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 10: Performance and Edge Cases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing edge cases...\n",
            "✓ Non-existent node handling: OK\n",
            "✓ Non-existent usage update handling: OK\n",
            "✓ Duplicate edge handling (update): OK\n",
            "✓ Invalid sentence offset handling: OK\n",
            "✓ Invalid embedding index handling: OK\n",
            "\n",
            "✓ All edge case tests passed\n"
          ]
        }
      ],
      "source": [
        "# Test edge cases\n",
        "print(\"Testing edge cases...\")\n",
        "\n",
        "# Test 1: Get non-existent node\n",
        "non_existent = get_node(99999, db_path=test_db_path)\n",
        "assert non_existent is None, \"Non-existent node should return None\"\n",
        "print(\"✓ Non-existent node handling: OK\")\n",
        "\n",
        "# Test 2: Update non-existent usage\n",
        "success = update_usage(99999, access_count=1, db_path=test_db_path)\n",
        "assert not success, \"Updating non-existent node should return False\"\n",
        "print(\"✓ Non-existent usage update handling: OK\")\n",
        "\n",
        "# Test 3: Add duplicate edge (should update, not fail)\n",
        "if len(created_node_ids) >= 2:\n",
        "    u, v = created_node_ids[0], created_node_ids[1]\n",
        "    success1 = add_graph_edge(u, v, 0.5, 0, db_path=test_db_path)\n",
        "    success2 = add_graph_edge(u, v, 0.7, 0, db_path=test_db_path)  # Same edge, different weight\n",
        "    assert success1 and success2, \"Adding duplicate edge should succeed (update)\"\n",
        "    \n",
        "    # Verify weight was updated\n",
        "    node = get_node(u, db_path=test_db_path)\n",
        "    edge = next((n for n in node['neighbors'] if n['v'] == v), None)\n",
        "    assert edge is not None, \"Edge should exist\"\n",
        "    assert abs(edge['weight'] - 0.7) < 0.01, \"Edge weight should be updated\"\n",
        "    print(\"✓ Duplicate edge handling (update): OK\")\n",
        "\n",
        "# Test 4: Load sentence with invalid offset\n",
        "invalid_sentence = load_sentence(sentences_path, 99999) if os.path.exists(sentences_path) else None\n",
        "assert invalid_sentence is None, \"Invalid sentence offset should return None\"\n",
        "print(\"✓ Invalid sentence offset handling: OK\")\n",
        "\n",
        "# Test 5: Load embedding with invalid index\n",
        "invalid_embedding = load_embedding(embeddings_path, 99999) if os.path.exists(embeddings_path) else None\n",
        "assert invalid_embedding is None, \"Invalid embedding index should return None\"\n",
        "print(\"✓ Invalid embedding index handling: OK\")\n",
        "\n",
        "print(\"\\n✓ All edge case tests passed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "All storage system tests completed. The hybrid architecture (SQLite + JSONL + NumPy) is working correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "STORAGE SYSTEM TEST SUMMARY\n",
            "============================================================\n",
            "Database: test_memory.db\n",
            "Nodes created: 5\n",
            "Edges added: 3\n",
            "Usage records: 5\n",
            "\n",
            "Components tested:\n",
            "  ✓ Database initialization\n",
            "  ✓ Node creation\n",
            "  ✓ Node retrieval\n",
            "  ✓ Usage statistics updates\n",
            "  ✓ Graph edge operations\n",
            "  ✓ JSONL file operations\n",
            "  ✓ NumPy array operations\n",
            "  ✓ Data consistency\n",
            "  ✓ Edge cases\n",
            "\n",
            "✓ All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# Final summary\n",
        "print(\"=\" * 60)\n",
        "print(\"STORAGE SYSTEM TEST SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Database: {test_db_path}\")\n",
        "print(f\"Nodes created: {len(created_node_ids)}\")\n",
        "print(f\"Edges added: {Neighbors.select().count()}\")\n",
        "print(f\"Usage records: {NodesUsage.select().count()}\")\n",
        "print(\"\\nComponents tested:\")\n",
        "print(\"  ✓ Database initialization\")\n",
        "print(\"  ✓ Node creation\")\n",
        "print(\"  ✓ Node retrieval\")\n",
        "print(\"  ✓ Usage statistics updates\")\n",
        "print(\"  ✓ Graph edge operations\")\n",
        "print(\"  ✓ JSONL file operations\")\n",
        "print(\"  ✓ NumPy array operations\")\n",
        "print(\"  ✓ Data consistency\")\n",
        "print(\"  ✓ Edge cases\")\n",
        "print(\"\\n✓ All tests passed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 11: Test create_node Function (Full Workflow)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing create_node() with new files...\n",
            "  Creating node with create_node()...\n",
            "  ✓ Created new node 100\n",
            "  ✓ Files created successfully\n",
            "  ✓ File contents verified (1 sentence, 1 embedding of 1536 dims)\n",
            "  Retrieving node with get_node()...\n",
            "  ✓ Embedding values match\n",
            "  Node ID: 100\n",
            "  Sentence: This is a test sentence created by create_node fun...\n",
            "  Embedding dim: 1536\n",
            "  Embedding norm: 22.553902\n",
            "  Source: test_create\n",
            "  Tag: test\n",
            "✓ create_node full workflow test passed\n",
            "\n",
            "  Testing append functionality...\n",
            "  ✓ Append functionality verified\n",
            "\n",
            "  Cleaning up test files...\n",
            "✓ Test files cleaned up\n"
          ]
        }
      ],
      "source": [
        "# Test create_node with new files (full workflow test)\n",
        "print(\"Testing create_node() with new files...\")\n",
        "\n",
        "# Use separate test files for this test\n",
        "test_create_sentences = \"test_create_sentences.jsonl\"\n",
        "test_create_embeddings = \"test_create_embeddings.npy\"\n",
        "\n",
        "# Clean up if exists\n",
        "if os.path.exists(test_create_sentences):\n",
        "    os.remove(test_create_sentences)\n",
        "    print(\"  Cleaned up existing test_sentences.jsonl\")\n",
        "if os.path.exists(test_create_embeddings):\n",
        "    os.remove(test_create_embeddings)\n",
        "    print(\"  Cleaned up existing test_embeddings.npy\")\n",
        "\n",
        "# Verify files don't exist before creation\n",
        "assert not os.path.exists(test_create_sentences), \"Test sentences file should not exist\"\n",
        "assert not os.path.exists(test_create_embeddings), \"Test embeddings file should not exist\"\n",
        "\n",
        "# Create a new node using create_node (this will create new files)\n",
        "print(\"  Creating node with create_node()...\")\n",
        "test_embedding = np.random.rand(1536).astype(np.float32)\n",
        "test_embedding_norm = np.linalg.norm(test_embedding)\n",
        "\n",
        "new_node_id = create_node(\n",
        "    sentence_text=\"This is a test sentence created by create_node function.\",\n",
        "    embedding_vector=test_embedding,\n",
        "    metadata_dict={\n",
        "        'id': 100,  # Use a high ID to avoid conflicts\n",
        "        'source': 'test_create',\n",
        "        'tag': 'test',\n",
        "        'language': 'en',\n",
        "        'initial_context': 'Test context',\n",
        "    },\n",
        "    sentences_path=test_create_sentences,\n",
        "    embeddings_path=test_create_embeddings,\n",
        "    db_path=test_db_path,\n",
        ")\n",
        "\n",
        "print(f\"  ✓ Created new node {new_node_id}\")\n",
        "\n",
        "# Verify files were created\n",
        "assert os.path.exists(test_create_sentences), \"Test sentences file should be created\"\n",
        "assert os.path.exists(test_create_embeddings), \"Test embeddings file should be created\"\n",
        "print(\"  ✓ Files created successfully\")\n",
        "\n",
        "# Verify file contents\n",
        "sentence_count = count_sentences(test_create_sentences)\n",
        "assert sentence_count == 1, f\"Should have 1 sentence, got {sentence_count}\"\n",
        "\n",
        "emb_shape = get_embedding_shape(test_create_embeddings)\n",
        "assert emb_shape is not None, \"Embedding shape should be readable\"\n",
        "assert emb_shape[0] == 1, f\"Should have 1 embedding, got {emb_shape[0]}\"\n",
        "assert emb_shape[1] == 1536, f\"Should have 1536 dimensions, got {emb_shape[1]}\"\n",
        "print(f\"  ✓ File contents verified (1 sentence, 1 embedding of {emb_shape[1]} dims)\")\n",
        "\n",
        "# Verify the node was created correctly\n",
        "print(\"  Retrieving node with get_node()...\")\n",
        "new_node = get_node(\n",
        "    new_node_id,\n",
        "    sentences_path=test_create_sentences,\n",
        "    embeddings_path=test_create_embeddings,\n",
        "    db_path=test_db_path,\n",
        ")\n",
        "\n",
        "assert new_node is not None, \"New node should exist\"\n",
        "assert new_node['context'] is not None, \"Context should not be None\"\n",
        "assert new_node['usage'] is not None, \"Usage should not be None\"\n",
        "assert new_node['context']['id'] == 100, f\"Node ID should be 100, got {new_node['context']['id']}\"\n",
        "assert new_node['sentence'] is not None, \"Sentence should be loaded\"\n",
        "assert new_node['embedding'] is not None, \"Embedding should be loaded\"\n",
        "assert len(new_node['embedding']) == 1536, f\"Embedding dimension should be 1536, got {len(new_node['embedding'])}\"\n",
        "\n",
        "# Verify embedding values match\n",
        "retrieved_embedding = np.array(new_node['embedding'])\n",
        "assert np.allclose(retrieved_embedding, test_embedding), \"Embedding values should match\"\n",
        "print(\"  ✓ Embedding values match\")\n",
        "\n",
        "# Verify embedding norm\n",
        "assert abs(new_node['context']['embedding_norm'] - test_embedding_norm) < 1e-5, \\\n",
        "    f\"Embedding norm should match: {new_node['context']['embedding_norm']} vs {test_embedding_norm}\"\n",
        "\n",
        "# Verify metadata\n",
        "assert new_node['context']['source'] == 'test_create', \"Source should match\"\n",
        "assert new_node['context']['tag'] == 'test', \"Tag should match\"\n",
        "assert new_node['context']['language'] == 'en', \"Language should match\"\n",
        "assert new_node['context']['initial_context'] == 'Test context', \"Initial context should match\"\n",
        "\n",
        "print(f\"  Node ID: {new_node['context']['id']}\")\n",
        "print(f\"  Sentence: {new_node['sentence'].get('text', '')[:50]}...\")\n",
        "print(f\"  Embedding dim: {len(new_node['embedding'])}\")\n",
        "print(f\"  Embedding norm: {new_node['context']['embedding_norm']:.6f}\")\n",
        "print(f\"  Source: {new_node['context']['source']}\")\n",
        "print(f\"  Tag: {new_node['context']['tag']}\")\n",
        "print(\"✓ create_node full workflow test passed\")\n",
        "\n",
        "# Test appending a second node\n",
        "print(\"\\n  Testing append functionality...\")\n",
        "second_embedding = np.random.rand(1536).astype(np.float32)\n",
        "second_node_id = create_node(\n",
        "    sentence_text=\"Second test sentence for append test.\",\n",
        "    embedding_vector=second_embedding,\n",
        "    metadata_dict={\n",
        "        'id': 101,\n",
        "        'source': 'test_create',\n",
        "        'tag': 'test_append',\n",
        "    },\n",
        "    sentences_path=test_create_sentences,\n",
        "    embeddings_path=test_create_embeddings,\n",
        "    db_path=test_db_path,\n",
        ")\n",
        "\n",
        "# Verify second node\n",
        "assert second_node_id == 101, f\"Second node ID should be 101, got {second_node_id}\"\n",
        "sentence_count_after = count_sentences(test_create_sentences)\n",
        "assert sentence_count_after == 2, f\"Should have 2 sentences after append, got {sentence_count_after}\"\n",
        "\n",
        "emb_shape_after = get_embedding_shape(test_create_embeddings)\n",
        "assert emb_shape_after[0] == 2, f\"Should have 2 embeddings after append, got {emb_shape_after[0]}\"\n",
        "\n",
        "second_node = get_node(101, sentences_path=test_create_sentences, \n",
        "                       embeddings_path=test_create_embeddings, db_path=test_db_path)\n",
        "assert second_node is not None, \"Second node should exist\"\n",
        "assert second_node['context']['sentence_offset'] == 1, \"Second node should have offset 1\"\n",
        "assert second_node['context']['embedding_index'] == 1, \"Second node should have embedding_index 1\"\n",
        "print(\"  ✓ Append functionality verified\")\n",
        "\n",
        "# Clean up test files\n",
        "print(\"\\n  Cleaning up test files...\")\n",
        "if os.path.exists(test_create_sentences):\n",
        "    os.remove(test_create_sentences)\n",
        "if os.path.exists(test_create_embeddings):\n",
        "    os.remove(test_create_embeddings)\n",
        "print(\"✓ Test files cleaned up\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
