{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FAISS Index Tests\n",
        "\n",
        "This notebook tests FAISS index functionality:\n",
        "1. Load existing FAISS index\n",
        "2. Search for similar embeddings\n",
        "3. Compare search results with database queries\n",
        "4. Performance benchmarks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "import numpy as np\n",
        "from memory import (\n",
        "    load_faiss_index,\n",
        "    search_faiss_index,\n",
        "    get_index_stats,\n",
        "    load_embedding,\n",
        "    get_node,\n",
        "    count_sentences,\n",
        "    load_sentence,\n",
        ")\n",
        "import os\n",
        "\n",
        "print(\"All imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 1: Load FAISS Index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load FAISS index\n",
        "data_dir = \"data\"\n",
        "faiss_index_path = os.path.join(data_dir, \"faiss.index\")\n",
        "\n",
        "if not os.path.exists(faiss_index_path):\n",
        "    print(f\"FAISS index not found at {faiss_index_path}\")\n",
        "    print(\"Run process_text.py first to generate the index\")\n",
        "else:\n",
        "    faiss_index = load_faiss_index(faiss_index_path)\n",
        "    stats = get_index_stats(faiss_index)\n",
        "    \n",
        "    print(\"FAISS Index Statistics:\")\n",
        "    print(f\"  Type: {stats['index_type']}\")\n",
        "    print(f\"  Number of vectors: {stats['num_vectors']}\")\n",
        "    print(f\"  Embedding dimension: {stats['embedding_dim']}\")\n",
        "    print(f\"  Is trained: {stats['is_trained']}\")\n",
        "    \n",
        "    if stats['index_type'] == 'ivf-pq':\n",
        "        print(f\"  nlist: {stats['nlist']}\")\n",
        "        print(f\"  m: {stats['m']}\")\n",
        "        print(f\"  nbits: {stats['nbits']}\")\n",
        "    \n",
        "    print(\"\\n[OK] FAISS index loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 2: Search for Similar Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load embeddings and test search\n",
        "embeddings_path = os.path.join(data_dir, \"embeddings.npy\")\n",
        "sentences_path = os.path.join(data_dir, \"sentences.jsonl\")\n",
        "\n",
        "if os.path.exists(embeddings_path) and os.path.exists(faiss_index_path):\n",
        "    embeddings = np.load(embeddings_path, mmap_mode='r')\n",
        "    faiss_index = load_faiss_index(faiss_index_path)\n",
        "    \n",
        "    # Test search with first embedding\n",
        "    query_embedding = embeddings[0]\n",
        "    k = 10\n",
        "    \n",
        "    print(f\"Searching for top {k} neighbors of first sentence...\")\n",
        "    distances, indices = search_faiss_index(faiss_index, query_embedding, k=k)\n",
        "    \n",
        "    print(f\"\\nTop {k} nearest neighbors:\")\n",
        "    print(\"-\" * 60)\n",
        "    for i, (idx, dist) in enumerate(zip(indices, distances)):\n",
        "        sentence = load_sentence(sentences_path, idx)\n",
        "        if sentence:\n",
        "            text = sentence.get('text', '')[:60]\n",
        "            print(f\"{i+1}. Index {idx}: distance={dist:.4f}\")\n",
        "            print(f\"   Text: {text}...\")\n",
        "        else:\n",
        "            print(f\"{i+1}. Index {idx}: distance={dist:.4f} (sentence not found)\")\n",
        "    \n",
        "    print(\"\\n[OK] FAISS search test passed\")\n",
        "else:\n",
        "    print(\"[SKIP] Required files not found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 3: Search with Query Embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test search with a new query embedding (average of first 5 sentences)\n",
        "if os.path.exists(embeddings_path) and os.path.exists(faiss_index_path):\n",
        "    embeddings = np.load(embeddings_path, mmap_mode='r')\n",
        "    faiss_index = load_faiss_index(faiss_index_path)\n",
        "    \n",
        "    # Create query embedding as average of first 5 sentences\n",
        "    query_emb = np.mean(embeddings[:5], axis=0).astype(np.float32)\n",
        "    \n",
        "    print(\"Searching with average embedding of first 5 sentences...\")\n",
        "    k = 10\n",
        "    distances, indices = search_faiss_index(faiss_index, query_emb, k=k)\n",
        "    \n",
        "    print(f\"\\nTop {k} results:\")\n",
        "    print(\"-\" * 60)\n",
        "    for i, (idx, dist) in enumerate(zip(indices, distances)):\n",
        "        sentence = load_sentence(sentences_path, idx)\n",
        "        if sentence:\n",
        "            text = sentence.get('text', '')[:60]\n",
        "            print(f\"{i+1}. Index {idx}: distance={dist:.4f}\")\n",
        "            print(f\"   Text: {text}...\")\n",
        "    \n",
        "    print(\"\\n[OK] Query embedding search test passed\")\n",
        "else:\n",
        "    print(\"[SKIP] Required files not found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 4: Compare FAISS Search with Database\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare FAISS search results with database node retrieval\n",
        "from memory import get_node, init_db\n",
        "\n",
        "if os.path.exists(embeddings_path) and os.path.exists(faiss_index_path):\n",
        "    embeddings = np.load(embeddings_path, mmap_mode='r')\n",
        "    faiss_index = load_faiss_index(faiss_index_path)\n",
        "    db_path = os.path.join(data_dir, \"memory.db\")\n",
        "    init_db(db_path)\n",
        "    \n",
        "    # Search for neighbors of node 1\n",
        "    test_node_id = 1\n",
        "    node = get_node(test_node_id, sentences_path=sentences_path, \n",
        "                   embeddings_path=embeddings_path, db_path=db_path)\n",
        "    \n",
        "    if node and node['embedding']:\n",
        "        query_emb = np.array(node['embedding'], dtype=np.float32)\n",
        "        \n",
        "        # FAISS search\n",
        "        k = 5\n",
        "        distances, indices = search_faiss_index(faiss_index, query_emb, k=k)\n",
        "        \n",
        "        print(f\"Node {test_node_id}: {node['sentence']['text'][:50]}...\")\n",
        "        print(f\"\\nFAISS top {k} neighbors (by embedding similarity):\")\n",
        "        for i, (idx, dist) in enumerate(zip(indices, distances)):\n",
        "            # Convert embedding index to node ID (assuming 1-based)\n",
        "            neighbor_node_id = idx + 1\n",
        "            neighbor_node = get_node(neighbor_node_id, sentences_path=sentences_path,\n",
        "                                   embeddings_path=embeddings_path, db_path=db_path)\n",
        "            if neighbor_node:\n",
        "                print(f\"  {i+1}. Node {neighbor_node_id}: distance={dist:.4f}\")\n",
        "                print(f\"     Text: {neighbor_node['sentence']['text'][:50]}...\")\n",
        "        \n",
        "        # Database graph neighbors\n",
        "        print(f\"\\nDatabase graph neighbors (by edge connections):\")\n",
        "        for i, neighbor in enumerate(node['neighbors'][:k]):\n",
        "            neighbor_node = get_node(neighbor['v'], sentences_path=sentences_path,\n",
        "                                   embeddings_path=embeddings_path, db_path=db_path)\n",
        "            if neighbor_node:\n",
        "                print(f\"  {i+1}. Node {neighbor['v']}: weight={neighbor['weight']:.2f}, type={neighbor['edge_type']}\")\n",
        "                print(f\"     Text: {neighbor_node['sentence']['text'][:50]}...\")\n",
        "        \n",
        "        print(\"\\n[OK] Comparison test completed\")\n",
        "    else:\n",
        "        print(\"[ERROR] Could not retrieve test node\")\n",
        "else:\n",
        "    print(\"[SKIP] Required files not found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 5: Performance Benchmark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark FAISS search performance\n",
        "import time\n",
        "\n",
        "if os.path.exists(embeddings_path) and os.path.exists(faiss_index_path):\n",
        "    embeddings = np.load(embeddings_path, mmap_mode='r')\n",
        "    faiss_index = load_faiss_index(faiss_index_path)\n",
        "    \n",
        "    num_queries = 100\n",
        "    k = 10\n",
        "    \n",
        "    print(f\"Running {num_queries} searches with k={k}...\")\n",
        "    \n",
        "    # Warm up\n",
        "    search_faiss_index(faiss_index, embeddings[0], k=k)\n",
        "    \n",
        "    # Benchmark\n",
        "    start_time = time.time()\n",
        "    for i in range(num_queries):\n",
        "        query_idx = i % len(embeddings)\n",
        "        query_emb = embeddings[query_idx]\n",
        "        distances, indices = search_faiss_index(faiss_index, query_emb, k=k)\n",
        "    end_time = time.time()\n",
        "    \n",
        "    total_time = end_time - start_time\n",
        "    avg_time = total_time / num_queries\n",
        "    \n",
        "    print(f\"\\nPerformance Results:\")\n",
        "    print(f\"  Total time: {total_time:.4f} seconds\")\n",
        "    print(f\"  Average time per query: {avg_time*1000:.2f} ms\")\n",
        "    print(f\"  Queries per second: {num_queries/total_time:.1f}\")\n",
        "    print(f\"  Index size: {faiss_index.ntotal} vectors\")\n",
        "    \n",
        "    print(\"\\n[OK] Performance benchmark completed\")\n",
        "else:\n",
        "    print(\"[SKIP] Required files not found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 6: Batch Search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test batch search (multiple queries at once)\n",
        "if os.path.exists(embeddings_path) and os.path.exists(faiss_index_path):\n",
        "    embeddings = np.load(embeddings_path, mmap_mode='r')\n",
        "    faiss_index = load_faiss_index(faiss_index_path)\n",
        "    \n",
        "    # Create batch of query embeddings\n",
        "    batch_size = 5\n",
        "    query_batch = embeddings[:batch_size].astype(np.float32)\n",
        "    k = 3\n",
        "    \n",
        "    print(f\"Batch search: {batch_size} queries, k={k}\")\n",
        "    \n",
        "    # FAISS supports batch search natively\n",
        "    distances, indices = faiss_index.search(query_batch, k)\n",
        "    \n",
        "    print(f\"\\nBatch search results:\")\n",
        "    print(\"-\" * 60)\n",
        "    for query_idx in range(batch_size):\n",
        "        print(f\"\\nQuery {query_idx + 1}:\")\n",
        "        for i, (idx, dist) in enumerate(zip(indices[query_idx], distances[query_idx])):\n",
        "            sentence = load_sentence(sentences_path, idx)\n",
        "            if sentence:\n",
        "                text = sentence.get('text', '')[:50]\n",
        "                print(f\"  {i+1}. Index {idx}: distance={dist:.4f} - {text}...\")\n",
        "    \n",
        "    print(\"\\n[OK] Batch search test passed\")\n",
        "else:\n",
        "    print(\"[SKIP] Required files not found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "FAISS index tests completed. The index provides fast similarity search for embeddings.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
