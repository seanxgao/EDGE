{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Memory Bacon: Embedding Exploration\n",
        "\n",
        "This notebook documents the embedding generation process for the `memory_bacon` dataset.\n",
        "\n",
        "## What We Did Today\n",
        "\n",
        "1. **Parsed essay text** (`3essay.txt`) into structured sentences with chapter and position metadata\n",
        "2. **Generated embeddings** using OpenAI's `text-embedding-3-small` model (1536 dimensions)\n",
        "3. **Computed contextual statistics** including similarity with neighboring sentences\n",
        "4. **Built graph edges** connecting sentences based on adjacency, questions, and definitions\n",
        "5. **Organized data** into a structured directory: `core/`, `graph/`, and `meta/`\n",
        "\n",
        "## Dataset Overview\n",
        "\n",
        "- **99 sentences** from 3 chapters\n",
        "- **1536-dimensional embeddings** per sentence\n",
        "- **201 graph edges** connecting related sentences\n",
        "- **Contextual features** including similarity scores with neighbors\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data directories:\n",
            "  Core: memory_bacon\\core\n",
            "  Graph: memory_bacon\\graph\n",
            "  Meta: memory_bacon\\meta\n",
            "\n",
            "[OK] All required files found\n"
          ]
        }
      ],
      "source": [
        "# Setup and imports\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from numpy.linalg import norm\n",
        "from numpy import dot\n",
        "\n",
        "# Load data paths\n",
        "BASE_DIR = Path(\"memory_bacon\")\n",
        "CORE_DIR = BASE_DIR / \"core\"\n",
        "GRAPH_DIR = BASE_DIR / \"graph\"\n",
        "META_DIR = BASE_DIR / \"meta\"\n",
        "\n",
        "print(\"Data directories:\")\n",
        "print(f\"  Core: {CORE_DIR}\")\n",
        "print(f\"  Graph: {GRAPH_DIR}\")\n",
        "print(f\"  Meta: {META_DIR}\")\n",
        "\n",
        "# Verify files exist\n",
        "assert (CORE_DIR / \"sentences.jsonl\").exists(), \"sentences.jsonl not found\"\n",
        "assert (CORE_DIR / \"embeddings.npy\").exists(), \"embeddings.npy not found\"\n",
        "assert (CORE_DIR / \"statistics.jsonl\").exists(), \"statistics.jsonl not found\"\n",
        "assert (META_DIR / \"embedding_meta.json\").exists(), \"embedding_meta.json not found\"\n",
        "\n",
        "print(\"\\n[OK] All required files found\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding Metadata:\n",
            "  Model: text-embedding-3-small\n",
            "  Timestamp: 2025-11-12 04:18:39\n",
            "  Number of sentences: 99\n",
            "  Embedding dimension: 1536\n",
            "\n",
            "Embeddings array shape: (99, 1536)\n",
            "  - Rows (sentences): 99\n",
            "  - Columns (dimensions): 1536\n",
            "  - Data type: float32\n",
            "  - Memory size: 608,256 bytes (0.58 MB)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'model': 'text-embedding-3-small',\n",
              " 'timestamp': '2025-11-12 04:18:39',\n",
              " 'num_sentences': 99,\n",
              " 'embedding_dim': 1536}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load metadata and overview\n",
        "with open(META_DIR / \"embedding_meta.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "print(\"Embedding Metadata:\")\n",
        "print(f\"  Model: {meta['model']}\")\n",
        "print(f\"  Timestamp: {meta['timestamp']}\")\n",
        "print(f\"  Number of sentences: {meta['num_sentences']}\")\n",
        "print(f\"  Embedding dimension: {meta['embedding_dim']}\")\n",
        "\n",
        "# Load embeddings\n",
        "embeddings = np.load(CORE_DIR / \"embeddings.npy\")\n",
        "print(f\"\\nEmbeddings array shape: {embeddings.shape}\")\n",
        "print(f\"  - Rows (sentences): {embeddings.shape[0]}\")\n",
        "print(f\"  - Columns (dimensions): {embeddings.shape[1]}\")\n",
        "print(f\"  - Data type: {embeddings.dtype}\")\n",
        "print(f\"  - Memory size: {embeddings.nbytes:,} bytes ({embeddings.nbytes / 1024 / 1024:.2f} MB)\")\n",
        "\n",
        "meta\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Embeddings\n",
        "\n",
        "Embeddings are dense vector representations that capture semantic meaning. Each sentence is mapped to a 1536-dimensional vector where:\n",
        "\n",
        "- **Similar sentences** have vectors that are close together (high cosine similarity)\n",
        "- **Different sentences** have vectors that are far apart (low cosine similarity)\n",
        "- **Each dimension** captures some aspect of meaning (though individual dimensions are not directly interpretable)\n",
        "\n",
        "Let's examine a few example sentences and their embeddings:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Example Sentences:\n",
            "\n",
            "[0] bacon_001 (Chapter 1, Position 1)\n",
            "     Text: What is truth? said jesting Pilate, and would not stay for an answer....\n",
            "\n",
            "[1] bacon_002 (Chapter 1, Position 2)\n",
            "     Text: Certainly there be, that delight in giddiness, and count it a bondage to fix a b...\n",
            "\n",
            "[8] bacon_009 (Chapter 1, Position 9)\n",
            "     Text: Doth any man doubt, that if there were taken out of men's minds, vain opinions, ...\n",
            "\n",
            "[22] bacon_023 (Chapter 2, Position 1)\n",
            "     Text: Men fear death, as children fear to go in the dark; and as that natural fear in ...\n",
            "\n",
            "[47] bacon_048 (Chapter 3, Position 1)\n",
            "     Text: Religion being the chief band of human society, is a happy thing, when itself is...\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'index': 0,\n",
              "  'id': 'bacon_001',\n",
              "  'chapter': 1,\n",
              "  'position': 1,\n",
              "  'text': 'What is truth? said jesting Pilate, and would not stay for an answer.',\n",
              "  'embedding': array([-0.02344766,  0.03554578, -0.03653585, ...,  0.01509247,\n",
              "          0.03518356,  0.02172108], shape=(1536,), dtype=float32)},\n",
              " {'index': 1,\n",
              "  'id': 'bacon_002',\n",
              "  'chapter': 1,\n",
              "  'position': 2,\n",
              "  'text': 'Certainly there be, that delight in giddiness, and count it a bondage to fix a belief; affecting free-will in thinking, as well as in acting.',\n",
              "  'embedding': array([ 0.01569673,  0.02020729,  0.0174038 , ...,  0.03453004,\n",
              "         -0.01758423,  0.01815325], shape=(1536,), dtype=float32)},\n",
              " {'index': 8,\n",
              "  'id': 'bacon_009',\n",
              "  'chapter': 1,\n",
              "  'position': 9,\n",
              "  'text': \"Doth any man doubt, that if there were taken out of men's minds, vain opinions, flattering hopes, false valuations, imaginations as one would, and the like, but it would leave the minds, of a number of men, poor shrunken things, full of melancholy and indisposition, and unpleasing to themselves?\",\n",
              "  'embedding': array([ 0.0223111 ,  0.0329605 ,  0.00590171, ...,  0.02999108,\n",
              "         -0.00441025,  0.02052945], shape=(1536,), dtype=float32)},\n",
              " {'index': 22,\n",
              "  'id': 'bacon_023',\n",
              "  'chapter': 2,\n",
              "  'position': 1,\n",
              "  'text': 'Men fear death, as children fear to go in the dark; and as that natural fear in children, is increased with tales, so is the other.',\n",
              "  'embedding': array([ 0.01123054,  0.05560138, -0.00787315, ...,  0.00778642,\n",
              "          0.02107352,  0.02024346], shape=(1536,), dtype=float32)},\n",
              " {'index': 47,\n",
              "  'id': 'bacon_048',\n",
              "  'chapter': 3,\n",
              "  'position': 1,\n",
              "  'text': 'Religion being the chief band of human society, is a happy thing, when itself is well contained within the true band of unity.',\n",
              "  'embedding': array([ 0.01152579, -0.00314505,  0.04220064, ...,  0.01786437,\n",
              "          0.01388913,  0.02200929], shape=(1536,), dtype=float32)}]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load sentences and select examples\n",
        "sentences = []\n",
        "with open(CORE_DIR / \"sentences.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        sentences.append(json.loads(line))\n",
        "\n",
        "# Select interesting examples\n",
        "example_indices = [0, 1, 8, 22, 47]  # First, second, question, chapter break, definition-like\n",
        "\n",
        "print(\"Selected Example Sentences:\\n\")\n",
        "examples = []\n",
        "for idx in example_indices:\n",
        "    sent = sentences[idx]\n",
        "    emb = embeddings[idx]\n",
        "    examples.append({\n",
        "        \"index\": idx,\n",
        "        \"id\": sent[\"id\"],\n",
        "        \"chapter\": sent[\"chapter\"],\n",
        "        \"position\": sent[\"position\"],\n",
        "        \"text\": sent[\"text\"],\n",
        "        \"embedding\": emb\n",
        "    })\n",
        "    print(f\"[{idx}] {sent['id']} (Chapter {sent['chapter']}, Position {sent['position']})\")\n",
        "    print(f\"     Text: {sent['text'][:80]}...\")\n",
        "    print()\n",
        "\n",
        "examples\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding for: What is truth? said jesting Pilate, and would not stay for a...\n",
            "\n",
            "Basic Statistics:\n",
            "  Shape: (1536,)\n",
            "  Min value: -0.0793\n",
            "  Max value: 0.0906\n",
            "  Mean: -0.0001\n",
            "  Std: 0.0255\n",
            "  Norm (length): 1.0000\n",
            "\n",
            "First 10 dimensions:\n",
            "  dim[   0] =  -0.0234\n",
            "  dim[   1] =   0.0355\n",
            "  dim[   2] =  -0.0365\n",
            "  dim[   3] =   0.0289\n",
            "  dim[   4] =  -0.0269\n",
            "  dim[   5] =   0.0027\n",
            "  dim[   6] =  -0.0496\n",
            "  dim[   7] =  -0.0257\n",
            "  dim[   8] =  -0.0022\n",
            "  dim[   9] =   0.0010\n",
            "\n",
            "Last 10 dimensions:\n",
            "  dim[1526] =   0.0224\n",
            "  dim[1527] =  -0.0026\n",
            "  dim[1528] =  -0.0008\n",
            "  dim[1529] =   0.0305\n",
            "  dim[1530] =  -0.0178\n",
            "  dim[1531] =  -0.0018\n",
            "  dim[1532] =  -0.0077\n",
            "  dim[1533] =   0.0151\n",
            "  dim[1534] =   0.0352\n",
            "  dim[1535] =   0.0217\n"
          ]
        }
      ],
      "source": [
        "# Examine embedding properties for first example\n",
        "first_emb = examples[0][\"embedding\"]\n",
        "\n",
        "print(f\"Embedding for: {examples[0]['text'][:60]}...\")\n",
        "print(f\"\\nBasic Statistics:\")\n",
        "print(f\"  Shape: {first_emb.shape}\")\n",
        "print(f\"  Min value: {first_emb.min():.4f}\")\n",
        "print(f\"  Max value: {first_emb.max():.4f}\")\n",
        "print(f\"  Mean: {first_emb.mean():.4f}\")\n",
        "print(f\"  Std: {first_emb.std():.4f}\")\n",
        "print(f\"  Norm (length): {norm(first_emb):.4f}\")\n",
        "\n",
        "print(f\"\\nFirst 10 dimensions:\")\n",
        "for i in range(10):\n",
        "    print(f\"  dim[{i:4d}] = {first_emb[i]:8.4f}\")\n",
        "\n",
        "print(f\"\\nLast 10 dimensions:\")\n",
        "for i in range(len(first_emb) - 10, len(first_emb)):\n",
        "    print(f\"  dim[{i:4d}] = {first_emb[i]:8.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What Do Embeddings Represent?\n",
        "\n",
        "The embedding vector captures semantic meaning in a high-dimensional space. While individual dimensions aren't directly interpretable, the **relationships between vectors** are meaningful:\n",
        "\n",
        "- **Cosine similarity** measures how similar two sentences are semantically\n",
        "- **Euclidean distance** can also measure similarity (though cosine is more common for embeddings)\n",
        "- **Clustering** in this space groups semantically related sentences\n",
        "\n",
        "Let's compute similarities between our example sentences:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity Matrix (Example Sentences):\n",
            "\n",
            "                                 bacon_001   bacon_002   bacon_009   bacon_023   bacon_048\n",
            "   bacon_001 What is truth? ...      1.0000      0.2458      0.3058      0.1623      0.2190\n",
            "   bacon_002 Certainly there...      0.2458      1.0000      0.4520      0.2786      0.3321\n",
            "   bacon_009 Doth any man do...      0.3058      0.4520      1.0000      0.2953      0.2362\n",
            "   bacon_023 Men fear death,...      0.1623      0.2786      0.2953      1.0000      0.1638\n",
            "   bacon_048 Religion being ...      0.2190      0.3321      0.2362      0.1638      1.0000\n",
            "\n",
            "Most similar pair:\n",
            "  bacon_002: Certainly there be, that delight in giddiness, and count it ...\n",
            "  bacon_009: Doth any man doubt, that if there were taken out of men's mi...\n",
            "  Similarity: 0.4520\n"
          ]
        }
      ],
      "source": [
        "# Compute cosine similarity between example sentences\n",
        "def cosine_similarity(a, b):\n",
        "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
        "    if norm(a) * norm(b) == 0:\n",
        "        return 0.0\n",
        "    return float(dot(a, b) / (norm(a) * norm(b)))\n",
        "\n",
        "print(\"Cosine Similarity Matrix (Example Sentences):\\n\")\n",
        "print(\" \" * 30, end=\"\")\n",
        "for ex in examples:\n",
        "    print(f\"{ex['id']:>12}\", end=\"\")\n",
        "print()\n",
        "\n",
        "similarity_matrix = []\n",
        "for i, ex1 in enumerate(examples):\n",
        "    row = []\n",
        "    print(f\"{ex1['id']:>12} {ex1['text'][:15]:>15}...\", end=\"\")\n",
        "    for j, ex2 in enumerate(examples):\n",
        "        sim = cosine_similarity(ex1[\"embedding\"], ex2[\"embedding\"])\n",
        "        row.append(sim)\n",
        "        print(f\"{sim:>12.4f}\", end=\"\")\n",
        "    similarity_matrix.append(row)\n",
        "    print()\n",
        "\n",
        "# Find most similar pair\n",
        "max_sim = -1\n",
        "max_pair = None\n",
        "for i in range(len(examples)):\n",
        "    for j in range(i + 1, len(examples)):\n",
        "        sim = similarity_matrix[i][j]\n",
        "        if sim > max_sim:\n",
        "            max_sim = sim\n",
        "            max_pair = (i, j)\n",
        "\n",
        "if max_pair:\n",
        "    print(f\"\\nMost similar pair:\")\n",
        "    print(f\"  {examples[max_pair[0]]['id']}: {examples[max_pair[0]]['text'][:60]}...\")\n",
        "    print(f\"  {examples[max_pair[1]]['id']}: {examples[max_pair[1]]['text'][:60]}...\")\n",
        "    print(f\"  Similarity: {max_sim:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contextual Statistics (First 5 Sentences):\n",
            "\n",
            "[0] bacon_001: What is truth? said jesting Pilate, and would not ...\n",
            "    sim_prev: None\n",
            "    sim_next: 0.2458\n",
            "    context_avg: 0.2458\n",
            "    context_delta: None\n",
            "\n",
            "[1] bacon_002: Certainly there be, that delight in giddiness, and...\n",
            "    sim_prev: 0.2458\n",
            "    sim_next: 0.3099\n",
            "    context_avg: 0.2778\n",
            "    context_delta: 0.0641\n",
            "\n",
            "[2] bacon_003: And though the sects of philosophers of that kind ...\n",
            "    sim_prev: 0.3099\n",
            "    sim_next: 0.3168\n",
            "    context_avg: 0.3134\n",
            "    context_delta: 0.0069\n",
            "\n",
            "[3] bacon_004: But it is not only the difficulty and labor, which...\n",
            "    sim_prev: 0.3168\n",
            "    sim_next: 0.6\n",
            "    context_avg: 0.4584\n",
            "    context_delta: 0.2832\n",
            "\n",
            "[4] bacon_005: One of the later school of the Grecians, examineth...\n",
            "    sim_prev: 0.6\n",
            "    sim_next: 0.357\n",
            "    context_avg: 0.4785\n",
            "    context_delta: 0.243\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load contextual statistics to see neighbor similarities\n",
        "statistics = []\n",
        "with open(CORE_DIR / \"statistics.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        statistics.append(json.loads(line))\n",
        "\n",
        "# Show statistics for first few sentences\n",
        "print(\"Contextual Statistics (First 5 Sentences):\\n\")\n",
        "for i in range(5):\n",
        "    stat = statistics[i]\n",
        "    sent = sentences[i]\n",
        "    print(f\"[{i}] {stat['id']}: {sent['text'][:50]}...\")\n",
        "    print(f\"    sim_prev: {stat['sim_prev']}\")\n",
        "    print(f\"    sim_next: {stat['sim_next']}\")\n",
        "    print(f\"    context_avg: {stat['context_avg']}\")\n",
        "    print(f\"    context_delta: {stat['context_delta']}\")\n",
        "    print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Statistics Dimensions Explained\n",
        "\n",
        "Each sentence in `statistics.jsonl` has **14 dimensions** that capture different aspects of the sentence. Let's break down what each dimension means with concrete examples:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "STATISTICS DIMENSIONS EXPLAINED\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Example 1: bacon_001\n",
            "Text: What is truth? said jesting Pilate, and would not stay for an answer.\n",
            "================================================================================\n",
            "\n",
            "1. BASIC STRUCTURAL FEATURES:\n",
            "   len_char = 69\n",
            "      → Number of characters in the sentence\n",
            "      → Example: 'What is truth? said jesting Pi...' has 69 characters\n",
            "\n",
            "   len_tok = 13\n",
            "      → Number of tokens (words) when split by whitespace\n",
            "      → Example: 'What is truth? said jesting Pi...' has 13 words\n",
            "\n",
            "   chapter = 1\n",
            "      → Which chapter this sentence belongs to (1-indexed)\n",
            "\n",
            "   position = 1\n",
            "      → Position of this sentence within its chapter (1-indexed)\n",
            "\n",
            "2. CONTENT FEATURES:\n",
            "   is_question = 0\n",
            "      → 0 means the sentence does NOT end with '?'\n",
            "      → Example: 'What is truth? said jesting Pilate, and would not ...' does not end with '?'\n",
            "\n",
            "   is_definition_like = 1\n",
            "      → 1 means the sentence contains definition patterns\n",
            "      → Patterns: ' is ', ' means ', ' refers to ' (case-insensitive)\n",
            "      → Example: 'What is truth? said jesting Pilate, and would not stay for a...' contains a definition pattern\n",
            "\n",
            "3. STYLISTIC FEATURES:\n",
            "   punct_density = 0.0435\n",
            "      → Ratio of punctuation marks to total characters\n",
            "      → Formula: (count of .,;:!?-\"()[] etc.) / len_char\n",
            "      → Range: 0.0 (no punctuation) to ~0.1+ (very punctuated)\n",
            "      → Example: 0.0435 means 4.35% of characters are punctuation\n",
            "\n",
            "   rel_pos_in_chapter = 0.0455\n",
            "      → Relative position within the chapter (0.0 to 1.0)\n",
            "      → Formula: position / total_sentences_in_chapter\n",
            "      → 0.0 = first sentence, 1.0 = last sentence, 0.5 = middle\n",
            "      → Example: 0.0455 means this is 4.5% through the chapter\n",
            "\n",
            "4. CONTEXTUAL SIMILARITY FEATURES:\n",
            "   sim_prev = None\n",
            "      → null because this is the FIRST sentence in the chapter\n",
            "      → No previous sentence to compare with\n",
            "\n",
            "   sim_next = 0.2458\n",
            "      → Cosine similarity with the NEXT sentence in the same chapter\n",
            "      → Range: -1.0 to 1.0 (typically 0.2 to 0.8 for related text)\n",
            "      → Higher values = more semantically similar to next sentence\n",
            "      → Example: 0.2458 means moderate similarity with next\n",
            "\n",
            "   context_avg = 0.2458\n",
            "      → Average of sim_prev and sim_next\n",
            "      → Formula: (sim_prev + sim_next) / 2\n",
            "      → Measures overall contextual similarity (stability indicator)\n",
            "      → High values = sentence fits well in its context\n",
            "      → Example: 0.2458 means average similarity with neighbors\n",
            "\n",
            "   context_delta = None\n",
            "      → null when only one neighbor exists (can't compute difference)\n",
            "\n",
            "5. POSITIONAL FEATURES:\n",
            "   is_paragraph_start = 1\n",
            "      → 1 means this sentence starts a new paragraph\n",
            "      → Either: position == 1 (chapter start) OR previous line was blank\n",
            "      → Example: 'What is truth? said jesting Pilate, and would not ...' starts a paragraph\n",
            "\n",
            "   is_paragraph_end = 0\n",
            "      → 0 means this sentence continues a paragraph\n",
            "\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Example 2: bacon_009\n",
            "Text: Doth any man doubt, that if there were taken out of men's minds, vain opinions, flattering hopes, false valuations, imaginations as one would, and the like, but it would leave the minds, of a number of men, poor shrunken things, full of melancholy and indisposition, and unpleasing to themselves?\n",
            "================================================================================\n",
            "\n",
            "1. BASIC STRUCTURAL FEATURES:\n",
            "   len_char = 296\n",
            "      → Number of characters in the sentence\n",
            "      → Example: 'Doth any man doubt, that if th...' has 296 characters\n",
            "\n",
            "   len_tok = 49\n",
            "      → Number of tokens (words) when split by whitespace\n",
            "      → Example: 'Doth any man doubt, that if th...' has 49 words\n",
            "\n",
            "   chapter = 1\n",
            "      → Which chapter this sentence belongs to (1-indexed)\n",
            "\n",
            "   position = 9\n",
            "      → Position of this sentence within its chapter (1-indexed)\n",
            "\n",
            "2. CONTENT FEATURES:\n",
            "   is_question = 1\n",
            "      → 1 means the sentence ends with '?' (it's a question)\n",
            "      → Example: 'Doth any man doubt, that if there were taken out of men's minds, vain opinions, flattering hopes, false valuations, imaginations as one would, and the like, but it would leave the minds, of a number of men, poor shrunken things, full of melancholy and indisposition, and unpleasing to themselves?' ends with '?'\n",
            "\n",
            "   is_definition_like = 0\n",
            "      → 0 means no definition patterns found\n",
            "      → Example: 'Doth any man doubt, that if there were taken out o...' does not contain definition patterns\n",
            "\n",
            "3. STYLISTIC FEATURES:\n",
            "   punct_density = 0.0439\n",
            "      → Ratio of punctuation marks to total characters\n",
            "      → Formula: (count of .,;:!?-\"()[] etc.) / len_char\n",
            "      → Range: 0.0 (no punctuation) to ~0.1+ (very punctuated)\n",
            "      → Example: 0.0439 means 4.39% of characters are punctuation\n",
            "\n",
            "   rel_pos_in_chapter = 0.4091\n",
            "      → Relative position within the chapter (0.0 to 1.0)\n",
            "      → Formula: position / total_sentences_in_chapter\n",
            "      → 0.0 = first sentence, 1.0 = last sentence, 0.5 = middle\n",
            "      → Example: 0.4091 means this is 40.9% through the chapter\n",
            "\n",
            "4. CONTEXTUAL SIMILARITY FEATURES:\n",
            "   sim_prev = 0.3744\n",
            "      → Cosine similarity with the PREVIOUS sentence in the same chapter\n",
            "      → Range: -1.0 to 1.0 (typically 0.2 to 0.8 for related text)\n",
            "      → Higher values = more semantically similar to previous sentence\n",
            "      → Example: 0.3744 means moderate similarity with previous\n",
            "\n",
            "   sim_next = 0.451\n",
            "      → Cosine similarity with the NEXT sentence in the same chapter\n",
            "      → Range: -1.0 to 1.0 (typically 0.2 to 0.8 for related text)\n",
            "      → Higher values = more semantically similar to next sentence\n",
            "      → Example: 0.4510 means moderate similarity with next\n",
            "\n",
            "   context_avg = 0.4127\n",
            "      → Average of sim_prev and sim_next\n",
            "      → Formula: (sim_prev + sim_next) / 2\n",
            "      → Measures overall contextual similarity (stability indicator)\n",
            "      → High values = sentence fits well in its context\n",
            "      → Example: 0.4127 means average similarity with neighbors\n",
            "\n",
            "   context_delta = 0.0766\n",
            "      → Absolute difference between sim_next and sim_prev\n",
            "      → Formula: |sim_next - sim_prev|\n",
            "      → Measures asymmetry in contextual similarity\n",
            "      → Low values = similar to both neighbors (balanced)\n",
            "      → High values = more similar to one neighbor than the other (transition point)\n",
            "      → Example: 0.0766 means balanced context\n",
            "\n",
            "5. POSITIONAL FEATURES:\n",
            "   is_paragraph_start = 0\n",
            "      → 0 means this sentence continues a paragraph\n",
            "\n",
            "   is_paragraph_end = 0\n",
            "      → 0 means this sentence continues a paragraph\n",
            "\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Example 3: bacon_022\n",
            "Text: Surely the wickedness of falsehood, and breach of faith, cannot possibly be so highly expressed, as in that it shall be the last peal, to call the judgments of God upon the generations of men; it being foretold, that when Christ cometh, he shall not find faith upon the earth.\n",
            "================================================================================\n",
            "\n",
            "1. BASIC STRUCTURAL FEATURES:\n",
            "   len_char = 276\n",
            "      → Number of characters in the sentence\n",
            "      → Example: 'Surely the wickedness of false...' has 276 characters\n",
            "\n",
            "   len_tok = 50\n",
            "      → Number of tokens (words) when split by whitespace\n",
            "      → Example: 'Surely the wickedness of false...' has 50 words\n",
            "\n",
            "   chapter = 1\n",
            "      → Which chapter this sentence belongs to (1-indexed)\n",
            "\n",
            "   position = 22\n",
            "      → Position of this sentence within its chapter (1-indexed)\n",
            "\n",
            "2. CONTENT FEATURES:\n",
            "   is_question = 0\n",
            "      → 0 means the sentence does NOT end with '?'\n",
            "      → Example: 'Surely the wickedness of falsehood, and breach of ...' does not end with '?'\n",
            "\n",
            "   is_definition_like = 0\n",
            "      → 0 means no definition patterns found\n",
            "      → Example: 'Surely the wickedness of falsehood, and breach of ...' does not contain definition patterns\n",
            "\n",
            "3. STYLISTIC FEATURES:\n",
            "   punct_density = 0.0290\n",
            "      → Ratio of punctuation marks to total characters\n",
            "      → Formula: (count of .,;:!?-\"()[] etc.) / len_char\n",
            "      → Range: 0.0 (no punctuation) to ~0.1+ (very punctuated)\n",
            "      → Example: 0.0290 means 2.90% of characters are punctuation\n",
            "\n",
            "   rel_pos_in_chapter = 1.0000\n",
            "      → Relative position within the chapter (0.0 to 1.0)\n",
            "      → Formula: position / total_sentences_in_chapter\n",
            "      → 0.0 = first sentence, 1.0 = last sentence, 0.5 = middle\n",
            "      → Example: 1.0000 means this is 100.0% through the chapter\n",
            "\n",
            "4. CONTEXTUAL SIMILARITY FEATURES:\n",
            "   sim_prev = 0.4374\n",
            "      → Cosine similarity with the PREVIOUS sentence in the same chapter\n",
            "      → Range: -1.0 to 1.0 (typically 0.2 to 0.8 for related text)\n",
            "      → Higher values = more semantically similar to previous sentence\n",
            "      → Example: 0.4374 means moderate similarity with previous\n",
            "\n",
            "   sim_next = None\n",
            "      → null because this is the LAST sentence in the chapter\n",
            "      → No next sentence to compare with\n",
            "\n",
            "   context_avg = 0.4374\n",
            "      → Average of sim_prev and sim_next\n",
            "      → Formula: (sim_prev + sim_next) / 2\n",
            "      → Measures overall contextual similarity (stability indicator)\n",
            "      → High values = sentence fits well in its context\n",
            "      → Example: 0.4374 means average similarity with neighbors\n",
            "\n",
            "   context_delta = None\n",
            "      → null when only one neighbor exists (can't compute difference)\n",
            "\n",
            "5. POSITIONAL FEATURES:\n",
            "   is_paragraph_start = 0\n",
            "      → 0 means this sentence continues a paragraph\n",
            "\n",
            "   is_paragraph_end = 1\n",
            "      → 1 means this sentence ends a paragraph\n",
            "      → Either: last sentence in chapter OR next line is blank\n",
            "      → Example: 'Surely the wickedness of falsehood, and breach of ...' ends a paragraph\n",
            "\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Example 4: bacon_023\n",
            "Text: Men fear death, as children fear to go in the dark; and as that natural fear in children, is increased with tales, so is the other.\n",
            "================================================================================\n",
            "\n",
            "1. BASIC STRUCTURAL FEATURES:\n",
            "   len_char = 131\n",
            "      → Number of characters in the sentence\n",
            "      → Example: 'Men fear death, as children fe...' has 131 characters\n",
            "\n",
            "   len_tok = 26\n",
            "      → Number of tokens (words) when split by whitespace\n",
            "      → Example: 'Men fear death, as children fe...' has 26 words\n",
            "\n",
            "   chapter = 2\n",
            "      → Which chapter this sentence belongs to (1-indexed)\n",
            "\n",
            "   position = 1\n",
            "      → Position of this sentence within its chapter (1-indexed)\n",
            "\n",
            "2. CONTENT FEATURES:\n",
            "   is_question = 0\n",
            "      → 0 means the sentence does NOT end with '?'\n",
            "      → Example: 'Men fear death, as children fear to go in the dark...' does not end with '?'\n",
            "\n",
            "   is_definition_like = 1\n",
            "      → 1 means the sentence contains definition patterns\n",
            "      → Patterns: ' is ', ' means ', ' refers to ' (case-insensitive)\n",
            "      → Example: 'Men fear death, as children fear to go in the dark; and as t...' contains a definition pattern\n",
            "\n",
            "3. STYLISTIC FEATURES:\n",
            "   punct_density = 0.0382\n",
            "      → Ratio of punctuation marks to total characters\n",
            "      → Formula: (count of .,;:!?-\"()[] etc.) / len_char\n",
            "      → Range: 0.0 (no punctuation) to ~0.1+ (very punctuated)\n",
            "      → Example: 0.0382 means 3.82% of characters are punctuation\n",
            "\n",
            "   rel_pos_in_chapter = 0.0400\n",
            "      → Relative position within the chapter (0.0 to 1.0)\n",
            "      → Formula: position / total_sentences_in_chapter\n",
            "      → 0.0 = first sentence, 1.0 = last sentence, 0.5 = middle\n",
            "      → Example: 0.0400 means this is 4.0% through the chapter\n",
            "\n",
            "4. CONTEXTUAL SIMILARITY FEATURES:\n",
            "   sim_prev = None\n",
            "      → null because this is the FIRST sentence in the chapter\n",
            "      → No previous sentence to compare with\n",
            "\n",
            "   sim_next = 0.4945\n",
            "      → Cosine similarity with the NEXT sentence in the same chapter\n",
            "      → Range: -1.0 to 1.0 (typically 0.2 to 0.8 for related text)\n",
            "      → Higher values = more semantically similar to next sentence\n",
            "      → Example: 0.4945 means moderate similarity with next\n",
            "\n",
            "   context_avg = 0.4945\n",
            "      → Average of sim_prev and sim_next\n",
            "      → Formula: (sim_prev + sim_next) / 2\n",
            "      → Measures overall contextual similarity (stability indicator)\n",
            "      → High values = sentence fits well in its context\n",
            "      → Example: 0.4945 means average similarity with neighbors\n",
            "\n",
            "   context_delta = None\n",
            "      → null when only one neighbor exists (can't compute difference)\n",
            "\n",
            "5. POSITIONAL FEATURES:\n",
            "   is_paragraph_start = 1\n",
            "      → 1 means this sentence starts a new paragraph\n",
            "      → Either: position == 1 (chapter start) OR previous line was blank\n",
            "      → Example: 'Men fear death, as children fear to go in the dark...' starts a paragraph\n",
            "\n",
            "   is_paragraph_end = 0\n",
            "      → 0 means this sentence continues a paragraph\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Detailed explanation of each statistics dimension\n",
        "# Let's load a few diverse examples to illustrate\n",
        "\n",
        "example_stats = [\n",
        "    statistics[0],   # First sentence (chapter start, definition-like)\n",
        "    statistics[8],   # Question sentence\n",
        "    statistics[21],  # Middle sentence with both neighbors\n",
        "    statistics[22],  # Chapter end\n",
        "]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STATISTICS DIMENSIONS EXPLAINED\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "for i, stat in enumerate(example_stats):\n",
        "    sent_idx = int(stat['id'].replace('bacon_', '')) - 1\n",
        "    sent_text = sentences[sent_idx]['text']\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Example {i+1}: {stat['id']}\")\n",
        "    print(f\"Text: {sent_text}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    # Basic structural features\n",
        "    print(\"1. BASIC STRUCTURAL FEATURES:\")\n",
        "    print(f\"   len_char = {stat['len_char']}\")\n",
        "    print(f\"      → Number of characters in the sentence\")\n",
        "    print(f\"      → Example: '{sent_text[:30]}...' has {stat['len_char']} characters\")\n",
        "    print()\n",
        "    \n",
        "    print(f\"   len_tok = {stat['len_tok']}\")\n",
        "    print(f\"      → Number of tokens (words) when split by whitespace\")\n",
        "    print(f\"      → Example: '{sent_text[:30]}...' has {stat['len_tok']} words\")\n",
        "    print()\n",
        "    \n",
        "    print(f\"   chapter = {stat['chapter']}\")\n",
        "    print(f\"      → Which chapter this sentence belongs to (1-indexed)\")\n",
        "    print()\n",
        "    \n",
        "    print(f\"   position = {stat['position']}\")\n",
        "    print(f\"      → Position of this sentence within its chapter (1-indexed)\")\n",
        "    print()\n",
        "    \n",
        "    # Content features\n",
        "    print(\"2. CONTENT FEATURES:\")\n",
        "    print(f\"   is_question = {stat['is_question']}\")\n",
        "    if stat['is_question'] == 1:\n",
        "        print(f\"      → 1 means the sentence ends with '?' (it's a question)\")\n",
        "        print(f\"      → Example: '{sent_text}' ends with '?'\")\n",
        "    else:\n",
        "        print(f\"      → 0 means the sentence does NOT end with '?'\")\n",
        "        print(f\"      → Example: '{sent_text[:50]}...' does not end with '?'\")\n",
        "    print()\n",
        "    \n",
        "    print(f\"   is_definition_like = {stat['is_definition_like']}\")\n",
        "    if stat['is_definition_like'] == 1:\n",
        "        print(f\"      → 1 means the sentence contains definition patterns\")\n",
        "        print(f\"      → Patterns: ' is ', ' means ', ' refers to ' (case-insensitive)\")\n",
        "        print(f\"      → Example: '{sent_text[:60]}...' contains a definition pattern\")\n",
        "    else:\n",
        "        print(f\"      → 0 means no definition patterns found\")\n",
        "        print(f\"      → Example: '{sent_text[:50]}...' does not contain definition patterns\")\n",
        "    print()\n",
        "    \n",
        "    # Stylistic features\n",
        "    print(\"3. STYLISTIC FEATURES:\")\n",
        "    print(f\"   punct_density = {stat['punct_density']:.4f}\")\n",
        "    print(f\"      → Ratio of punctuation marks to total characters\")\n",
        "    print(f\"      → Formula: (count of .,;:!?-\\\"()[] etc.) / len_char\")\n",
        "    print(f\"      → Range: 0.0 (no punctuation) to ~0.1+ (very punctuated)\")\n",
        "    print(f\"      → Example: {stat['punct_density']:.4f} means {stat['punct_density']*100:.2f}% of characters are punctuation\")\n",
        "    print()\n",
        "    \n",
        "    print(f\"   rel_pos_in_chapter = {stat['rel_pos_in_chapter']:.4f}\")\n",
        "    print(f\"      → Relative position within the chapter (0.0 to 1.0)\")\n",
        "    print(f\"      → Formula: position / total_sentences_in_chapter\")\n",
        "    print(f\"      → 0.0 = first sentence, 1.0 = last sentence, 0.5 = middle\")\n",
        "    print(f\"      → Example: {stat['rel_pos_in_chapter']:.4f} means this is {stat['rel_pos_in_chapter']*100:.1f}% through the chapter\")\n",
        "    print()\n",
        "    \n",
        "    # Contextual similarity features\n",
        "    print(\"4. CONTEXTUAL SIMILARITY FEATURES:\")\n",
        "    print(f\"   sim_prev = {stat['sim_prev']}\")\n",
        "    if stat['sim_prev'] is not None:\n",
        "        print(f\"      → Cosine similarity with the PREVIOUS sentence in the same chapter\")\n",
        "        print(f\"      → Range: -1.0 to 1.0 (typically 0.2 to 0.8 for related text)\")\n",
        "        print(f\"      → Higher values = more semantically similar to previous sentence\")\n",
        "        print(f\"      → Example: {stat['sim_prev']:.4f} means moderate similarity with previous\")\n",
        "    else:\n",
        "        print(f\"      → null because this is the FIRST sentence in the chapter\")\n",
        "        print(f\"      → No previous sentence to compare with\")\n",
        "    print()\n",
        "    \n",
        "    print(f\"   sim_next = {stat['sim_next']}\")\n",
        "    if stat['sim_next'] is not None:\n",
        "        print(f\"      → Cosine similarity with the NEXT sentence in the same chapter\")\n",
        "        print(f\"      → Range: -1.0 to 1.0 (typically 0.2 to 0.8 for related text)\")\n",
        "        print(f\"      → Higher values = more semantically similar to next sentence\")\n",
        "        print(f\"      → Example: {stat['sim_next']:.4f} means moderate similarity with next\")\n",
        "    else:\n",
        "        print(f\"      → null because this is the LAST sentence in the chapter\")\n",
        "        print(f\"      → No next sentence to compare with\")\n",
        "    print()\n",
        "    \n",
        "    print(f\"   context_avg = {stat['context_avg']}\")\n",
        "    if stat['context_avg'] is not None:\n",
        "        print(f\"      → Average of sim_prev and sim_next\")\n",
        "        print(f\"      → Formula: (sim_prev + sim_next) / 2\")\n",
        "        print(f\"      → Measures overall contextual similarity (stability indicator)\")\n",
        "        print(f\"      → High values = sentence fits well in its context\")\n",
        "        print(f\"      → Example: {stat['context_avg']:.4f} means average similarity with neighbors\")\n",
        "    else:\n",
        "        print(f\"      → null when only one neighbor exists (chapter start or end)\")\n",
        "        print(f\"      → In that case, equals the single available similarity\")\n",
        "    print()\n",
        "    \n",
        "    print(f\"   context_delta = {stat['context_delta']}\")\n",
        "    if stat['context_delta'] is not None:\n",
        "        print(f\"      → Absolute difference between sim_next and sim_prev\")\n",
        "        print(f\"      → Formula: |sim_next - sim_prev|\")\n",
        "        print(f\"      → Measures asymmetry in contextual similarity\")\n",
        "        print(f\"      → Low values = similar to both neighbors (balanced)\")\n",
        "        print(f\"      → High values = more similar to one neighbor than the other (transition point)\")\n",
        "        print(f\"      → Example: {stat['context_delta']:.4f} means {'balanced' if stat['context_delta'] < 0.1 else 'asymmetric'} context\")\n",
        "    else:\n",
        "        print(f\"      → null when only one neighbor exists (can't compute difference)\")\n",
        "    print()\n",
        "    \n",
        "    # Positional features\n",
        "    print(\"5. POSITIONAL FEATURES:\")\n",
        "    print(f\"   is_paragraph_start = {stat['is_paragraph_start']}\")\n",
        "    if stat['is_paragraph_start'] == 1:\n",
        "        print(f\"      → 1 means this sentence starts a new paragraph\")\n",
        "        print(f\"      → Either: position == 1 (chapter start) OR previous line was blank\")\n",
        "        print(f\"      → Example: '{sent_text[:50]}...' starts a paragraph\")\n",
        "    else:\n",
        "        print(f\"      → 0 means this sentence continues a paragraph\")\n",
        "    print()\n",
        "    \n",
        "    print(f\"   is_paragraph_end = {stat['is_paragraph_end']}\")\n",
        "    if stat['is_paragraph_end'] == 1:\n",
        "        print(f\"      → 1 means this sentence ends a paragraph\")\n",
        "        print(f\"      → Either: last sentence in chapter OR next line is blank\")\n",
        "        print(f\"      → Example: '{sent_text[:50]}...' ends a paragraph\")\n",
        "    else:\n",
        "        print(f\"      → 0 means this sentence continues a paragraph\")\n",
        "    print()\n",
        "    \n",
        "    if i < len(example_stats) - 1:\n",
        "        print(\"\\n\" + \"─\" * 80 + \"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Statistics Summary Table\n",
        "\n",
        "Here's a quick reference table for all statistics dimensions:\n",
        "\n",
        "| Dimension | Type | Range | Description |\n",
        "|-----------|------|-------|-------------|\n",
        "| `id` | string | - | Sentence identifier (e.g., \"bacon_001\") |\n",
        "| `chapter` | int | 1+ | Chapter number |\n",
        "| `position` | int | 1+ | Position within chapter |\n",
        "| `len_char` | int | 1+ | Number of characters |\n",
        "| `len_tok` | int | 1+ | Number of tokens (words) |\n",
        "| `is_question` | int | 0 or 1 | 1 if ends with \"?\" |\n",
        "| `is_definition_like` | int | 0 or 1 | 1 if contains \" is \", \" means \", \" refers to \" |\n",
        "| `punct_density` | float | 0.0-1.0 | Punctuation count / character count |\n",
        "| `rel_pos_in_chapter` | float | 0.0-1.0 | position / total_sentences_in_chapter |\n",
        "| `sim_prev` | float/null | -1.0 to 1.0 | Cosine similarity with previous sentence |\n",
        "| `sim_next` | float/null | -1.0 to 1.0 | Cosine similarity with next sentence |\n",
        "| `context_avg` | float/null | -1.0 to 1.0 | Average of sim_prev and sim_next |\n",
        "| `context_delta` | float/null | 0.0+ | Absolute difference |sim_next - sim_prev| |\n",
        "| `is_paragraph_start` | int | 0 or 1 | 1 if starts a paragraph |\n",
        "| `is_paragraph_end` | int | 0 or 1 | 1 if ends a paragraph |\n",
        "\n",
        "### Use Cases\n",
        "\n",
        "These statistics can be used for:\n",
        "- **XGBoost feature engineering**: All numeric features can be used as model inputs\n",
        "- **Graph edge weighting**: Use similarity scores to weight edges\n",
        "- **Content analysis**: Identify questions, definitions, and structural patterns\n",
        "- **Context understanding**: Measure how well sentences fit in their context\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity Statistics (first 20 sentences):\n",
            "  Total pairs: 190\n",
            "  Mean similarity: 0.3734\n",
            "  Std similarity: 0.0901\n",
            "  Min similarity: 0.1202\n",
            "  Max similarity: 0.6169\n",
            "  Median similarity: 0.3747\n",
            "\n",
            "Similarity Distribution:\n",
            "  < 0.2: 4 pairs\n",
            "  0.2-0.4: 117 pairs\n",
            "  0.4-0.6: 66 pairs\n",
            "  0.6-0.8: 3 pairs\n",
            "  >= 0.8: 0 pairs\n"
          ]
        }
      ],
      "source": [
        "# Compute all pairwise similarities (sample for efficiency)\n",
        "# For full dataset, this would be 99 * 99 = 9801 comparisons\n",
        "# Let's do a sample: first 20 sentences\n",
        "\n",
        "sample_size = 20\n",
        "sample_embs = embeddings[:sample_size]\n",
        "sample_sents = sentences[:sample_size]\n",
        "\n",
        "similarities = []\n",
        "for i in range(sample_size):\n",
        "    for j in range(i + 1, sample_size):\n",
        "        sim = cosine_similarity(sample_embs[i], sample_embs[j])\n",
        "        similarities.append(sim)\n",
        "\n",
        "similarities = np.array(similarities)\n",
        "\n",
        "print(f\"Similarity Statistics (first {sample_size} sentences):\")\n",
        "print(f\"  Total pairs: {len(similarities)}\")\n",
        "print(f\"  Mean similarity: {similarities.mean():.4f}\")\n",
        "print(f\"  Std similarity: {similarities.std():.4f}\")\n",
        "print(f\"  Min similarity: {similarities.min():.4f}\")\n",
        "print(f\"  Max similarity: {similarities.max():.4f}\")\n",
        "print(f\"  Median similarity: {np.median(similarities):.4f}\")\n",
        "\n",
        "# Show distribution\n",
        "print(f\"\\nSimilarity Distribution:\")\n",
        "print(f\"  < 0.2: {np.sum(similarities < 0.2)} pairs\")\n",
        "print(f\"  0.2-0.4: {np.sum((similarities >= 0.2) & (similarities < 0.4))} pairs\")\n",
        "print(f\"  0.4-0.6: {np.sum((similarities >= 0.4) & (similarities < 0.6))} pairs\")\n",
        "print(f\"  0.6-0.8: {np.sum((similarities >= 0.6) & (similarities < 0.8))} pairs\")\n",
        "print(f\"  >= 0.8: {np.sum(similarities >= 0.8)} pairs\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target sentence: What is truth? said jesting Pilate, and would not stay for an answer.\n",
            "\n",
            "Top 5 Most Similar Sentences:\n",
            "\n",
            "1. Similarity: 0.4772\n",
            "   bacon_085 (Chapter 3, Position 38)\n",
            "   For truth and falsehood, in such things, are like the iron and clay, in the toes of Nebuchadnezzar's image; they may cleave, but they will not incorporate.\n",
            "\n",
            "2. Similarity: 0.4413\n",
            "   bacon_007 (Chapter 1, Position 7)\n",
            "   Truth may perhaps come to the price of a pearl, that showeth best by day; but it will not rise to the price of a diamond, or carbuncle, that showeth best in varied lights.\n",
            "\n",
            "3. Similarity: 0.4284\n",
            "   bacon_017 (Chapter 1, Position 17)\n",
            "   To pass from theological, and philosophical truth, to the truth of civil business; it will be acknowledged, even by those that practise it not, that clear, and round dealing, is the honor of man's nature; and that mixture of falsehoods, is like alloy in coin of gold and silver, which may make the metal work the better, but it embaseth it.\n",
            "\n",
            "4. Similarity: 0.4217\n",
            "   bacon_012 (Chapter 1, Position 12)\n",
            "   But, howsoever these things are thus in men's depraved judgments, and affections, yet truth, which only doth judge itself, teacheth that the inquiry of truth, which is the love-making, or wooing of it, the knowledge of truth, which is the presence of it, and the belief of truth, which is the enjoying of it, is the sovereign good of human nature.\n",
            "\n",
            "5. Similarity: 0.4144\n",
            "   bacon_036 (Chapter 2, Position 14)\n",
            "   Tiberius in dissimulation; as Tacitus saith of him, Jam Tiberius vires et corpus, non dissimulationem, deserebant.\n",
            "\n",
            "\n",
            "Top 5 Least Similar Sentences:\n",
            "\n",
            "1. Similarity: 0.1363\n",
            "   bacon_047 (Chapter 2, Position 25)\n",
            "   Extinctus amabitur idem.\n",
            "\n",
            "2. Similarity: 0.1346\n",
            "   bacon_040 (Chapter 2, Position 18)\n",
            "   And the like.\n",
            "\n",
            "3. Similarity: 0.1263\n",
            "   bacon_074 (Chapter 3, Position 27)\n",
            "   But if it were done less partially, it would be embraced more generally.\n",
            "\n",
            "4. Similarity: 0.1240\n",
            "   bacon_075 (Chapter 3, Position 28)\n",
            "   Of this I may give only this advice, according to my small model.\n",
            "\n",
            "5. Similarity: 0.1177\n",
            "   bacon_067 (Chapter 3, Position 20)\n",
            "   There appear to be two extremes.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Find nearest neighbors for a specific sentence\n",
        "target_idx = 0  # First sentence\n",
        "target_emb = embeddings[target_idx]\n",
        "target_sent = sentences[target_idx]\n",
        "\n",
        "print(f\"Target sentence: {target_sent['text']}\\n\")\n",
        "\n",
        "# Compute similarities with all other sentences\n",
        "all_similarities = []\n",
        "for i in range(len(embeddings)):\n",
        "    if i != target_idx:\n",
        "        sim = cosine_similarity(target_emb, embeddings[i])\n",
        "        all_similarities.append((i, sim, sentences[i]))\n",
        "\n",
        "# Sort by similarity (descending)\n",
        "all_similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"Top 5 Most Similar Sentences:\\n\")\n",
        "for rank, (idx, sim, sent) in enumerate(all_similarities[:5], 1):\n",
        "    print(f\"{rank}. Similarity: {sim:.4f}\")\n",
        "    print(f\"   {sent['id']} (Chapter {sent['chapter']}, Position {sent['position']})\")\n",
        "    print(f\"   {sent['text']}\")\n",
        "    print()\n",
        "\n",
        "print(\"\\nTop 5 Least Similar Sentences:\\n\")\n",
        "for rank, (idx, sim, sent) in enumerate(all_similarities[-5:], 1):\n",
        "    print(f\"{rank}. Similarity: {sim:.4f}\")\n",
        "    print(f\"   {sent['id']} (Chapter {sent['chapter']}, Position {sent['position']})\")\n",
        "    print(f\"   {sent['text']}\")\n",
        "    print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### What Embeddings Enable\n",
        "\n",
        "1. **Semantic Search**: Find sentences similar to a query\n",
        "2. **Clustering**: Group related sentences together\n",
        "3. **Graph Construction**: Connect semantically similar sentences\n",
        "4. **Feature Engineering**: Use embedding dimensions as features for ML models\n",
        "\n",
        "### Key Insights\n",
        "\n",
        "- Each sentence is represented as a **1536-dimensional vector**\n",
        "- Similarity is measured via **cosine similarity** (normalized dot product)\n",
        "- Embeddings capture **semantic meaning**, not just word overlap\n",
        "- The model (`text-embedding-3-small`) was trained on large text corpora to learn these representations\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Use embeddings for **FAISS indexing** (fast similarity search)\n",
        "- Build **graph edges** based on semantic similarity thresholds\n",
        "- Train **XGBoost models** using embedding features + contextual statistics\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
